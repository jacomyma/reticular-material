<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Infomedia Articles</title>
    <style type="text/css">
/*!
 * Pico.css v1.3.3 (https://picocss.com)
 * Copyright 2019-2021 - Licensed under MIT
 */:root{--font-family:system-ui,-apple-system,"Segoe UI","Roboto","Ubuntu","Cantarell","Noto Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";--line-height:1.5;--font-weight:400;--font-size:16px;--border-radius:.25rem;--border-width:1px;--outline-width:3px;--spacing:1rem;--typography-spacing-vertical:1.5rem;--block-spacing-vertical:calc(var(--spacing) * 2);--block-spacing-horizontal:var(--spacing);--grid-spacing-vertical:0;--grid-spacing-horizontal:var(--spacing);--form-element-spacing-vertical:.75rem;--form-element-spacing-horizontal:1rem;--transition:.2s ease-in-out}@media (min-width:576px){:root{--font-size:17px}}@media (min-width:768px){:root{--font-size:18px}}@media (min-width:992px){:root{--font-size:19px}}@media (min-width:1200px){:root{--font-size:20px}}@media (min-width:576px){body>footer,body>header,body>main,section{--block-spacing-vertical:calc(var(--spacing) * 2.5)}}@media (min-width:768px){body>footer,body>header,body>main,section{--block-spacing-vertical:calc(var(--spacing) * 3)}}@media (min-width:992px){body>footer,body>header,body>main,section{--block-spacing-vertical:calc(var(--spacing) * 3.5)}}@media (min-width:1200px){body>footer,body>header,body>main,section{--block-spacing-vertical:calc(var(--spacing) * 4)}}@media (min-width:576px){article{--block-spacing-horizontal:calc(var(--spacing) * 1.25)}}@media (min-width:768px){article{--block-spacing-horizontal:calc(var(--spacing) * 1.5)}}@media (min-width:992px){article{--block-spacing-horizontal:calc(var(--spacing) * 1.75)}}@media (min-width:1200px){article{--block-spacing-horizontal:calc(var(--spacing) * 2)}}a{--text-decoration:none}a.contrast,a.secondary{--text-decoration:underline}small{--font-size:0.875em}h1,h2,h3,h4,h5,h6{--font-weight:700}h1{--font-size:2rem;--typography-spacing-vertical:3rem}h2{--font-size:1.75rem;--typography-spacing-vertical:2.625rem}h3{--font-size:1.5rem;--typography-spacing-vertical:2.25rem}h4{--font-size:1.25rem;--typography-spacing-vertical:1.874rem}h5{--font-size:1.125rem;--typography-spacing-vertical:1.6875rem}[type=checkbox],[type=radio]{--border-width:2px}[type=checkbox][role=switch]{--border-width:3px}thead td,thead th{--border-width:3px}:not(thead)>*>td{--font-size:0.875em}code,kbd,pre,samp{--font-family:"Menlo","Consolas","Roboto Mono","Ubuntu Monospace","Noto Mono","Oxygen Mono","Liberation Mono",monospace,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji"}kbd{--font-weight:bolder}:root:not([data-theme=dark]),[data-theme=light]{color-scheme:light;--background-color:#FFF;--color:#415462;--h1-color:#1b2832;--h2-color:#23333e;--h3-color:#2c3d49;--h4-color:#374956;--h5-color:#415462;--h6-color:#4d606d;--muted-color:#73828c;--muted-border-color:#edf0f3;--primary:#1095c1;--primary-hover:#08769b;--primary-focus:rgba(16,149,193,0.125);--primary-inverse:#FFF;--secondary:#596b78;--secondary-hover:#415462;--secondary-focus:rgba(89,107,120,0.125);--secondary-inverse:#FFF;--contrast:#1b2832;--contrast-hover:#000;--contrast-focus:rgba(89,107,120,0.125);--contrast-inverse:#FFF;--mark-background-color:#fff2ca;--mark-color:#543a25;--ins-color:#388E3C;--del-color:#C62828;--blockquote-border-color:var(--muted-border-color);--blockquote-footer-color:var(--muted-color);--button-box-shadow:0 0 0 rgba(0,0,0,0);--button-hover-box-shadow:0 0 0 rgba(0,0,0,0);--form-element-background-color:transparent;--form-element-border-color:#a2afb9;--form-element-color:var(--color);--form-element-placeholder-color:var(--muted-color);--form-element-active-background-color:transparent;--form-element-active-border-color:var(--primary);--form-element-focus-color:var(--primary-focus);--form-element-disabled-background-color:#d5dce2;--form-element-disabled-border-color:#a2afb9;--form-element-disabled-opacity:.5;--form-element-invalid-border-color:#C62828;--form-element-invalid-active-border-color:#B71C1C;--form-element-valid-border-color:#388E3C;--form-element-valid-active-border-color:#2E7D32;--switch-background-color:#bbc6ce;--switch-color:var(--primary-inverse);--switch-checked-background-color:var(--primary);--range-border-color:#d5dce2;--range-active-border-color:#bbc6ce;--range-thumb-border-color:var(--background-color);--range-thumb-color:var(--secondary);--range-thumb-hover-color:var(--secondary-hover);--range-thumb-active-color:var(--primary);--table-border-color:var(--muted-border-color);--table-row-stripped-background-color:#f6f8f9;--code-background-color:#edf0f3;--code-color:var(--muted-color);--code-kbd-background-color:var(--contrast);--code-kbd-color:var(--contrast-inverse);--code-tag-color:#b34d80;--code-property-color:#3d888f;--code-value-color:#998866;--code-comment-color:#a2afb9;--accordion-border-color:var(--muted-border-color);--accordion-close-summary-color:var(--color);--accordion-open-summary-color:var(--muted-color);--card-background-color:var(--background-color);--card-border-color:var(--muted-border-color);--card-box-shadow:0 0.125rem 1rem rgba(27,40,50,0.04),0 0.125rem 2rem rgba(27,40,50,0.08),0 0 0 0.0625rem rgba(27,40,50,0.024);--card-sectionning-background-color:#fafbfc;--progress-background-color:#d5dce2;--progress-color:var(--primary);--loading-spinner-opacity:.5;--tooltip-background-color:var(--contrast);--tooltip-color:var(--contrast-inverse);--icon-chevron:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgba(65, 84, 98, 0.999)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='6 9 12 15 18 9'%3E%3C/polyline%3E%3C/svg%3E");--icon-date:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgba(65, 84, 98, 0.999)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Crect x='3' y='4' width='18' height='18' rx='2' ry='2'%3E%3C/rect%3E%3Cline x1='16' y1='2' x2='16' y2='6'%3E%3C/line%3E%3Cline x1='8' y1='2' x2='8' y2='6'%3E%3C/line%3E%3Cline x1='3' y1='10' x2='21' y2='10'%3E%3C/line%3E%3C/svg%3E");--icon-time:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgba(65, 84, 98, 0.999)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Ccircle cx='12' cy='12' r='10'%3E%3C/circle%3E%3Cpolyline points='12 6 12 12 16 14'%3E%3C/polyline%3E%3C/svg%3E");--icon-search:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgba(65, 84, 98, 0.999)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Ccircle cx='11' cy='11' r='8'%3E%3C/circle%3E%3Cline x1='21' y1='21' x2='16.65' y2='16.65'%3E%3C/line%3E%3C/svg%3E");--icon-checkbox:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='%23FFF' stroke-width='4' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='20 6 9 17 4 12'%3E%3C/polyline%3E%3C/svg%3E");--icon-minus:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='%23FFF' stroke-width='4' stroke-linecap='round' stroke-linejoin='round'%3E%3Cline x1='5' y1='12' x2='19' y2='12'%3E%3C/line%3E%3C/svg%3E");--icon-valid:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgba(56, 142, 60, 0.999)' stroke-width='3' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='20 6 9 17 4 12'%3E%3C/polyline%3E%3C/svg%3E");--icon-invalid:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgba(198, 40, 40, 0.999)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Ccircle cx='12' cy='12' r='10'%3E%3C/circle%3E%3Cline x1='12' y1='8' x2='12' y2='12'%3E%3C/line%3E%3Cline x1='12' y1='16' x2='12.01' y2='16'%3E%3C/line%3E%3C/svg%3E")}@media only screen and (prefers-color-scheme:dark){:root:not([data-theme=light]){color-scheme:dark;--background-color:#11191f;--color:#bbc6ce;--h1-color:#edf0f3;--h2-color:#e1e6ea;--h3-color:#d5dce2;--h4-color:#c8d1d8;--h5-color:#bbc6ce;--h6-color:#aebbc3;--muted-color:#73828c;--muted-border-color:#1f2d38;--primary:#1095c1;--primary-hover:#1ab3e6;--primary-focus:rgba(16,149,193,0.25);--primary-inverse:#FFF;--secondary:#596b78;--secondary-hover:#73828c;--secondary-focus:rgba(115,130,140,0.25);--secondary-inverse:#FFF;--contrast:#edf0f3;--contrast-hover:#FFF;--contrast-focus:rgba(115,130,140,0.25);--contrast-inverse:#000;--mark-background-color:#d0c284;--mark-color:#11191f;--ins-color:#388E3C;--del-color:#C62828;--blockquote-border-color:var(--muted-border-color);--blockquote-footer-color:var(--muted-color);--button-box-shadow:0 0 0 rgba(0,0,0,0);--button-hover-box-shadow:0 0 0 rgba(0,0,0,0);--form-element-background-color:#11191f;--form-element-border-color:#374956;--form-element-color:var(--color);--form-element-placeholder-color:var(--muted-color);--form-element-active-background-color:var(--form-element-background-color);--form-element-active-border-color:var(--primary);--form-element-focus-color:var(--primary-focus);--form-element-disabled-background-color:#2c3d49;--form-element-disabled-border-color:#415462;--form-element-disabled-opacity:.5;--form-element-invalid-border-color:#B71C1C;--form-element-invalid-active-border-color:#C62828;--form-element-valid-border-color:#2E7D32;--form-element-valid-active-border-color:#388E3C;--switch-background-color:#374956;--switch-color:var(--primary-inverse);--switch-checked-background-color:var(--primary);--range-border-color:#23333e;--range-active-border-color:#2c3d49;--range-thumb-border-color:var(--background-color);--range-thumb-color:var(--secondary);--range-thumb-hover-color:var(--secondary-hover);--range-thumb-active-color:var(--primary);--table-border-color:var(--muted-border-color);--table-row-stripped-background-color:rgba(115,130,140,0.05);--code-background-color:#17232c;--code-color:var(--muted-color);--code-kbd-background-color:var(--contrast);--code-kbd-color:var(--contrast-inverse);--code-tag-color:#a65980;--code-property-color:#599fa6;--code-value-color:#8c8473;--code-comment-color:#4d606d;--accordion-border-color:var(--muted-border-color);--accordion-active-summary-color:var(--primary);--accordion-close-summary-color:var(--color);--accordion-open-summary-color:var(--muted-color);--card-background-color:#141e25;--card-border-color:#11191f;--card-box-shadow:0 0.125rem 1rem rgba(0,0,0,0.06),0 0.125rem 2rem rgba(0,0,0,0.12),0 0 0 0.0625rem rgba(0,0,0,0.036);--card-sectionning-background-color:#17232c;--progress-background-color:#23333e;--progress-color:var(--primary);--loading-spinner-opacity:.5;--tooltip-background-color:var(--contrast);--tooltip-color:var(--contrast-inverse);--icon-chevron:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgba(162, 175, 185, 0.999)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='6 9 12 15 18 9'%3E%3C/polyline%3E%3C/svg%3E");--icon-date:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgba(162, 175, 185, 0.999)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Crect x='3' y='4' width='18' height='18' rx='2' ry='2'%3E%3C/rect%3E%3Cline x1='16' y1='2' x2='16' y2='6'%3E%3C/line%3E%3Cline x1='8' y1='2' x2='8' y2='6'%3E%3C/line%3E%3Cline x1='3' y1='10' x2='21' y2='10'%3E%3C/line%3E%3C/svg%3E");--icon-time:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgba(162, 175, 185, 0.999)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Ccircle cx='12' cy='12' r='10'%3E%3C/circle%3E%3Cpolyline points='12 6 12 12 16 14'%3E%3C/polyline%3E%3C/svg%3E");--icon-search:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgba(162, 175, 185, 0.999)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Ccircle cx='11' cy='11' r='8'%3E%3C/circle%3E%3Cline x1='21' y1='21' x2='16.65' y2='16.65'%3E%3C/line%3E%3C/svg%3E");--icon-checkbox:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='%23FFF' stroke-width='4' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='20 6 9 17 4 12'%3E%3C/polyline%3E%3C/svg%3E");--icon-minus:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='%23FFF' stroke-width='4' stroke-linecap='round' stroke-linejoin='round'%3E%3Cline x1='5' y1='12' x2='19' y2='12'%3E%3C/line%3E%3C/svg%3E");--icon-valid:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgba(46, 125, 50, 0.999)' stroke-width='3' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='20 6 9 17 4 12'%3E%3C/polyline%3E%3C/svg%3E");--icon-invalid:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgba(183, 28, 28, 0.999)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Ccircle cx='12' cy='12' r='10'%3E%3C/circle%3E%3Cline x1='12' y1='8' x2='12' y2='12'%3E%3C/line%3E%3Cline x1='12' y1='16' x2='12.01' y2='16'%3E%3C/line%3E%3C/svg%3E")}}[data-theme=dark]{color-scheme:dark;--background-color:#11191f;--color:#bbc6ce;--h1-color:#edf0f3;--h2-color:#e1e6ea;--h3-color:#d5dce2;--h4-color:#c8d1d8;--h5-color:#bbc6ce;--h6-color:#aebbc3;--muted-color:#73828c;--muted-border-color:#1f2d38;--primary:#1095c1;--primary-hover:#1ab3e6;--primary-focus:rgba(16,149,193,0.25);--primary-inverse:#FFF;--secondary:#596b78;--secondary-hover:#73828c;--secondary-focus:rgba(115,130,140,0.25);--secondary-inverse:#FFF;--contrast:#edf0f3;--contrast-hover:#FFF;--contrast-focus:rgba(115,130,140,0.25);--contrast-inverse:#000;--mark-background-color:#d0c284;--mark-color:#11191f;--ins-color:#388E3C;--del-color:#C62828;--blockquote-border-color:var(--muted-border-color);--blockquote-footer-color:var(--muted-color);--button-box-shadow:0 0 0 rgba(0,0,0,0);--button-hover-box-shadow:0 0 0 rgba(0,0,0,0);--form-element-background-color:#11191f;--form-element-border-color:#374956;--form-element-color:var(--color);--form-element-placeholder-color:var(--muted-color);--form-element-active-background-color:var(--form-element-background-color);--form-element-active-border-color:var(--primary);--form-element-focus-color:var(--primary-focus);--form-element-disabled-background-color:#2c3d49;--form-element-disabled-border-color:#415462;--form-element-disabled-opacity:.5;--form-element-invalid-border-color:#B71C1C;--form-element-invalid-active-border-color:#C62828;--form-element-valid-border-color:#2E7D32;--form-element-valid-active-border-color:#388E3C;--switch-background-color:#374956;--switch-color:var(--primary-inverse);--switch-checked-background-color:var(--primary);--range-border-color:#23333e;--range-active-border-color:#2c3d49;--range-thumb-border-color:var(--background-color);--range-thumb-color:var(--secondary);--range-thumb-hover-color:var(--secondary-hover);--range-thumb-active-color:var(--primary);--table-border-color:var(--muted-border-color);--table-row-stripped-background-color:rgba(115,130,140,0.05);--code-background-color:#17232c;--code-color:var(--muted-color);--code-kbd-background-color:var(--contrast);--code-kbd-color:var(--contrast-inverse);--code-tag-color:#a65980;--code-property-color:#599fa6;--code-value-color:#8c8473;--code-comment-color:#4d606d;--accordion-border-color:var(--muted-border-color);--accordion-active-summary-color:var(--primary);--accordion-close-summary-color:var(--color);--accordion-open-summary-color:var(--muted-color);--card-background-color:#141e25;--card-border-color:#11191f;--card-box-shadow:0 0.125rem 1rem rgba(0,0,0,0.06),0 0.125rem 2rem rgba(0,0,0,0.12),0 0 0 0.0625rem rgba(0,0,0,0.036);--card-sectionning-background-color:#17232c;--progress-background-color:#23333e;--progress-color:var(--primary);--loading-spinner-opacity:.5;--tooltip-background-color:var(--contrast);--tooltip-color:var(--contrast-inverse);--icon-chevron:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgba(162, 175, 185, 0.999)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='6 9 12 15 18 9'%3E%3C/polyline%3E%3C/svg%3E");--icon-date:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgba(162, 175, 185, 0.999)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Crect x='3' y='4' width='18' height='18' rx='2' ry='2'%3E%3C/rect%3E%3Cline x1='16' y1='2' x2='16' y2='6'%3E%3C/line%3E%3Cline x1='8' y1='2' x2='8' y2='6'%3E%3C/line%3E%3Cline x1='3' y1='10' x2='21' y2='10'%3E%3C/line%3E%3C/svg%3E");--icon-time:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgba(162, 175, 185, 0.999)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Ccircle cx='12' cy='12' r='10'%3E%3C/circle%3E%3Cpolyline points='12 6 12 12 16 14'%3E%3C/polyline%3E%3C/svg%3E");--icon-search:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgba(162, 175, 185, 0.999)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Ccircle cx='11' cy='11' r='8'%3E%3C/circle%3E%3Cline x1='21' y1='21' x2='16.65' y2='16.65'%3E%3C/line%3E%3C/svg%3E");--icon-checkbox:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='%23FFF' stroke-width='4' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='20 6 9 17 4 12'%3E%3C/polyline%3E%3C/svg%3E");--icon-minus:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='%23FFF' stroke-width='4' stroke-linecap='round' stroke-linejoin='round'%3E%3Cline x1='5' y1='12' x2='19' y2='12'%3E%3C/line%3E%3C/svg%3E");--icon-valid:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgba(46, 125, 50, 0.999)' stroke-width='3' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='20 6 9 17 4 12'%3E%3C/polyline%3E%3C/svg%3E");--icon-invalid:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgba(183, 28, 28, 0.999)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Ccircle cx='12' cy='12' r='10'%3E%3C/circle%3E%3Cline x1='12' y1='8' x2='12' y2='12'%3E%3C/line%3E%3Cline x1='12' y1='16' x2='12.01' y2='16'%3E%3C/line%3E%3C/svg%3E")}*,:after,:before{box-sizing:border-box}:after,:before{text-decoration:inherit;vertical-align:inherit}html{-webkit-text-size-adjust:100%;-webkit-tap-highlight-color:rgba(0,0,0,0);-moz-tab-size:4;-ms-text-size-adjust:100%;background-color:var(--background-color);color:var(--color);font-family:var(--font-family);font-size:var(--font-size);font-weight:var(--font-weight);line-height:var(--line-height);text-rendering:optimizeLegibility;cursor:default}main{display:block}body{width:100%;margin:0}body>footer,body>header,body>main{width:100%;margin-right:auto;margin-left:auto;padding:var(--block-spacing-vertical) 0}.container,.container-fluid{width:100%;margin-right:auto;margin-left:auto;padding-right:var(--spacing);padding-left:var(--spacing)}@media (min-width:576px){.container{max-width:510px;padding-right:0;padding-left:0}}@media (min-width:768px){.container{max-width:700px}}@media (min-width:992px){.container{max-width:920px}}@media (min-width:1200px){.container{max-width:1130px}}section{margin-bottom:var(--block-spacing-vertical)}.grid{grid-column-gap:var(--grid-spacing-horizontal);grid-row-gap:var(--grid-spacing-vertical);display:grid;grid-template-columns:1fr;margin:0}@media (min-width:992px){.grid{grid-template-columns:repeat(auto-fit,minmax(0%,1fr))}}.grid>*{min-width:0}figure{display:block;margin:0;padding:0;overflow-x:auto}figure figcaption{padding:calc(var(--spacing) / 2) 0;color:var(--muted-color)}b,strong{font-weight:bolder}sub,sup{position:relative;font-size:.75em;line-height:0;vertical-align:baseline}sub{bottom:-0.25em}sup{top:-0.5em}dl dl,dl ol,dl ul,ol dl,ul dl{margin:0}ol ol,ol ul,ul ol,ul ul{margin:0}address,blockquote,dl,figure,form,ol,p,pre,table,ul{margin-top:0;margin-bottom:var(--typography-spacing-vertical);color:var(--color);font-size:1rem;font-style:normal}a{--color:var(--primary);--background-color:transparent;outline:none;background-color:var(--background-color);color:var(--color);text-decoration:var(--text-decoration);transition:background-color var(--transition),color var(--transition),text-decoration var(--transition),box-shadow var(--transition)}a:active,a:focus,a:hover{--color:var(--primary-hover);--text-decoration:underline}a:focus{--background-color:var(--primary-focus)}a.secondary{--color:var(--secondary)}a.secondary:active,a.secondary:focus,a.secondary:hover{--color:var(--secondary-hover)}a.secondary:focus{--background-color:var(--secondary-focus)}a.contrast{--color:var(--contrast)}a.contrast:active,a.contrast:focus,a.contrast:hover{--color:var(--contrast-hover)}a.contrast:focus{--background-color:var(--contrast-focus)}h1,h2,h3,h4,h5,h6{margin-top:0;margin-bottom:var(--typography-spacing-vertical);color:var(--color);font-family:var(--font-family);font-size:var(--font-size);font-weight:var(--font-weight)}h1{--color:var(--h1-color)}h2{--color:var(--h2-color)}h3{--color:var(--h3-color)}h4{--color:var(--h4-color)}h5{--color:var(--h5-color)}h6{--color:var(--h6-color)}address~h1,address~h2,address~h3,address~h4,address~h5,address~h6,blockquote~h1,blockquote~h2,blockquote~h3,blockquote~h4,blockquote~h5,blockquote~h6,dl~h1,dl~h2,dl~h3,dl~h4,dl~h5,dl~h6,figure~h1,figure~h2,figure~h3,figure~h4,figure~h5,figure~h6,form~h1,form~h2,form~h3,form~h4,form~h5,form~h6,ol~h1,ol~h2,ol~h3,ol~h4,ol~h5,ol~h6,pre~h1,pre~h2,pre~h3,pre~h4,pre~h5,pre~h6,p~h1,p~h2,p~h3,p~h4,p~h5,p~h6,table~h1,table~h2,table~h3,table~h4,table~h5,table~h6,ul~h1,ul~h2,ul~h3,ul~h4,ul~h5,ul~h6{margin-top:var(--typography-spacing-vertical)}hgroup{margin-bottom:var(--typography-spacing-vertical)}hgroup>*{margin-bottom:0}hgroup>:last-child{--color:var(--muted-color);--font-weight:unset;font-family:unset;font-size:1rem}p{margin-bottom:var(--typography-spacing-vertical)}small{font-size:var(--font-size)}ol,ul{padding-left:var(--spacing)}ol li,ul li{margin-bottom:calc(var(--typography-spacing-vertical) / 4)}ul li{list-style:square}mark{padding:.125rem .25rem;background-color:var(--mark-background-color);color:var(--mark-color);vertical-align:middle}blockquote{display:block;margin:var(--typography-spacing-vertical) 0;padding:var(--spacing);border-left:0.25rem solid var(--blockquote-border-color)}blockquote footer{margin-top:calc(var(--typography-spacing-vertical) / 2);color:var(--blockquote-footer-color)}abbr[title]{border-bottom:1px dotted;text-decoration:none;cursor:help}ins{color:var(--ins-color);text-decoration:none}del{color:var(--del-color)}::selection{background-color:var(--primary-focus)}audio,canvas,iframe,img,svg,video{vertical-align:middle}audio,video{display:inline-block}audio:not([controls]){display:none;height:0}iframe{border-style:none}img{max-width:100%;height:auto;border-style:none}svg:not([fill]){fill:currentColor}svg:not(:root){overflow:hidden}button{margin:0;overflow:visible;font-family:inherit;text-transform:none}[type=button],[type=reset],[type=submit],button{-webkit-appearance:button}[type=button]::-moz-focus-inner,[type=reset]::-moz-focus-inner,[type=submit]::-moz-focus-inner,button::-moz-focus-inner{padding:0;border-style:none}button{display:block;width:100%;margin-bottom:var(--spacing)}a[role=button]{display:inline-block;text-decoration:none}a[role=button],button,input[type=button],input[type=reset],input[type=submit]{--background-color:var(--primary);--border-color:var(--primary);--color:var(--primary-inverse);--box-shadow:var(--button-box-shadow,0 0 0 rgba(0,0,0,0));padding:var(--form-element-spacing-vertical) var(--form-element-spacing-horizontal);border:var(--border-width) solid var(--border-color);border-radius:var(--border-radius);outline:none;background-color:var(--background-color);box-shadow:var(--box-shadow);color:var(--color);font-size:1rem;font-weight:var(--font-weight);line-height:var(--line-height);text-align:center;cursor:pointer;transition:background-color var(--transition),border-color var(--transition),color var(--transition),box-shadow var(--transition)}a[role=button]:active,a[role=button]:focus,a[role=button]:hover,button:active,button:focus,button:hover,input[type=button]:active,input[type=button]:focus,input[type=button]:hover,input[type=reset]:active,input[type=reset]:focus,input[type=reset]:hover,input[type=submit]:active,input[type=submit]:focus,input[type=submit]:hover{--background-color:var(--primary-hover);--border-color:var(--primary-hover);--box-shadow:var(--button-hover-box-shadow,0 0 0 rgba(0,0,0,0))}a[role=button]:focus,button:focus,input[type=button]:focus,input[type=reset]:focus,input[type=submit]:focus{--box-shadow:var(--button-hover-box-shadow,0 0 0 rgba(0,0,0,0)),0 0 0 var(--outline-width) var(--primary-focus)}input[type=reset]{--background-color:var(--secondary);--border-color:var(--secondary);--color:var(--secondary-inverse);cursor:pointer}input[type=reset]:active,input[type=reset]:focus,input[type=reset]:hover{--background-color:var(--secondary-hover);--border-color:var(--secondary-hover)}input[type=reset]:focus{--box-shadow:var(--button-hover-box-shadow,0 0 0 rgba(0,0,0,0)),0 0 0 var(--outline-width) var(--secondary-focus)}a[role=button].secondary,button.secondary,input[type=button].secondary,input[type=reset].secondary,input[type=submit].secondary{--background-color:var(--secondary);--border-color:var(--secondary);--color:var(--secondary-inverse);cursor:pointer}a[role=button].secondary:active,a[role=button].secondary:focus,a[role=button].secondary:hover,button.secondary:active,button.secondary:focus,button.secondary:hover,input[type=button].secondary:active,input[type=button].secondary:focus,input[type=button].secondary:hover,input[type=reset].secondary:active,input[type=reset].secondary:focus,input[type=reset].secondary:hover,input[type=submit].secondary:active,input[type=submit].secondary:focus,input[type=submit].secondary:hover{--background-color:var(--secondary-hover);--border-color:var(--secondary-hover)}a[role=button].secondary:focus,button.secondary:focus,input[type=button].secondary:focus,input[type=reset].secondary:focus,input[type=submit].secondary:focus{--box-shadow:var(--button-hover-box-shadow,0 0 0 rgba(0,0,0,0)),0 0 0 var(--outline-width) var(--secondary-focus)}a[role=button].contrast,button.contrast,input[type=button].contrast,input[type=reset].contrast,input[type=submit].contrast{--background-color:var(--contrast);--border-color:var(--contrast);--color:var(--contrast-inverse)}a[role=button].contrast:active,a[role=button].contrast:focus,a[role=button].contrast:hover,button.contrast:active,button.contrast:focus,button.contrast:hover,input[type=button].contrast:active,input[type=button].contrast:focus,input[type=button].contrast:hover,input[type=reset].contrast:active,input[type=reset].contrast:focus,input[type=reset].contrast:hover,input[type=submit].contrast:active,input[type=submit].contrast:focus,input[type=submit].contrast:hover{--background-color:var(--contrast-hover);--border-color:var(--contrast-hover)}a[role=button].contrast:focus,button.contrast:focus,input[type=button].contrast:focus,input[type=reset].contrast:focus,input[type=submit].contrast:focus{--box-shadow:var(--button-hover-box-shadow,0 0 0 rgba(0,0,0,0)),0 0 0 var(--outline-width) var(--contrast-focus)}a[role=button].outline,button.outline,input[type=button].outline,input[type=reset].outline,input[type=submit].outline{--background-color:transparent;--color:var(--primary)}a[role=button].outline:active,a[role=button].outline:focus,a[role=button].outline:hover,button.outline:active,button.outline:focus,button.outline:hover,input[type=button].outline:active,input[type=button].outline:focus,input[type=button].outline:hover,input[type=reset].outline:active,input[type=reset].outline:focus,input[type=reset].outline:hover,input[type=submit].outline:active,input[type=submit].outline:focus,input[type=submit].outline:hover{--background-color:transparent;--color:var(--primary-hover)}a[role=button].outline.secondary,button.outline.secondary,input[type=button].outline.secondary,input[type=reset].outline.secondary,input[type=submit].outline.secondary{--color:var(--secondary)}a[role=button].outline.secondary:active,a[role=button].outline.secondary:focus,a[role=button].outline.secondary:hover,button.outline.secondary:active,button.outline.secondary:focus,button.outline.secondary:hover,input[type=button].outline.secondary:active,input[type=button].outline.secondary:focus,input[type=button].outline.secondary:hover,input[type=reset].outline.secondary:active,input[type=reset].outline.secondary:focus,input[type=reset].outline.secondary:hover,input[type=submit].outline.secondary:active,input[type=submit].outline.secondary:focus,input[type=submit].outline.secondary:hover{--color:var(--secondary-hover)}a[role=button].outline.contrast,button.outline.contrast,input[type=button].outline.contrast,input[type=reset].outline.contrast,input[type=submit].outline.contrast{--color:var(--contrast)}a[role=button].outline.contrast:active,a[role=button].outline.contrast:focus,a[role=button].outline.contrast:hover,button.outline.contrast:active,button.outline.contrast:focus,button.outline.contrast:hover,input[type=button].outline.contrast:active,input[type=button].outline.contrast:focus,input[type=button].outline.contrast:hover,input[type=reset].outline.contrast:active,input[type=reset].outline.contrast:focus,input[type=reset].outline.contrast:hover,input[type=submit].outline.contrast:active,input[type=submit].outline.contrast:focus,input[type=submit].outline.contrast:hover{--color:var(--contrast-hover)}a[role=button][disabled],button[disabled],input[type=button][disabled],input[type=reset][disabled],input[type=submit][disabled]{opacity:.5;pointer-events:none}input,optgroup,select,textarea{margin:0;font-family:inherit;font-size:1rem;letter-spacing:inherit;line-height:var(--line-height)}input{overflow:visible}select{text-transform:none}legend{max-width:100%;padding:0;color:inherit;white-space:normal}textarea{overflow:auto}[type=checkbox],[type=radio]{padding:0}::-webkit-inner-spin-button,::-webkit-outer-spin-button{height:auto}[type=search]{-webkit-appearance:textfield;outline-offset:-2px}[type=search]::-webkit-search-decoration{-webkit-appearance:none}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}::-moz-focus-inner{padding:0;border-style:none}:-moz-focusring{outline:none}:-moz-ui-invalid{box-shadow:none}::-ms-expand{display:none}[type=file],[type=range]{padding:0;border-width:0}input:not([type=checkbox]):not([type=radio]):not([type=range]){height:calc((1rem * var(--line-height)) + (var(--form-element-spacing-vertical) * 2) + (var(--border-width) * 2))}fieldset{margin:0;margin-bottom:var(--spacing);padding:0;border:0}fieldset legend,label{display:block;margin-bottom:calc(var(--spacing) / 4);vertical-align:middle}input:not([type=checkbox]):not([type=radio]),select,textarea{display:block;width:100%}input:not([type=checkbox]):not([type=radio]):not([type=range]):not([type=file]),select,textarea{-webkit-appearance:none;-moz-appearance:none;appearance:none;padding:var(--form-element-spacing-vertical) var(--form-element-spacing-horizontal);vertical-align:middle}input,select,textarea{--background-color:var(--form-element-background-color);--border-color:var(--form-element-border-color);--color:var(--form-element-color);--box-shadow:none;border:var(--border-width) solid var(--border-color);border-radius:var(--border-radius);outline:none;background-color:var(--background-color);box-shadow:var(--box-shadow);color:var(--color);font-weight:var(--font-weight);transition:background-color var(--transition),border-color var(--transition),color var(--transition),box-shadow var(--transition)}input:not([type=submit]):not([type=button]):not([type=reset]):not([type=checkbox]):not([type=radio]):not([readonly]):active,input:not([type=submit]):not([type=button]):not([type=reset]):not([type=checkbox]):not([type=radio]):not([readonly]):focus,select:active,select:focus,textarea:active,textarea:focus{--background-color:var(--form-element-active-background-color)}input:not([type=submit]):not([type=button]):not([type=reset]):not([role=switch]):not([readonly]):active,input:not([type=submit]):not([type=button]):not([type=reset]):not([role=switch]):not([readonly]):focus,select:active,select:focus,textarea:active,textarea:focus{--border-color:var(--form-element-active-border-color)}input:not([type=submit]):not([type=button]):not([type=reset]):not([type=range]):not([type=file]):not([readonly]):focus,select:focus,textarea:focus{--box-shadow:0 0 0 var(--outline-width) var(--form-element-focus-color)}input:not([type=submit]):not([type=button]):not([type=reset])[disabled],select[disabled],textarea[disabled]{--background-color:var(--form-element-disabled-background-color);--border-color:var(--form-element-disabled-border-color);opacity:var(--form-element-disabled-opacity)}input[aria-invalid],select[aria-invalid],textarea[aria-invalid]{padding-right:2rem;background-position:center right .75rem;background-repeat:no-repeat;background-size:1rem auto}input[aria-invalid=false],select[aria-invalid=false],textarea[aria-invalid=false]{--border-color:var(--form-element-valid-border-color);background-image:var(--icon-valid)}input[aria-invalid=false]:active,input[aria-invalid=false]:focus,select[aria-invalid=false]:active,select[aria-invalid=false]:focus,textarea[aria-invalid=false]:active,textarea[aria-invalid=false]:focus{--border-color:var(--form-element-valid-active-border-color)!important}input[aria-invalid=true],select[aria-invalid=true],textarea[aria-invalid=true]{--border-color:var(--form-element-invalid-border-color);background-image:var(--icon-invalid)}input[aria-invalid=true]:active,input[aria-invalid=true]:focus,select[aria-invalid=true]:active,select[aria-invalid=true]:focus,textarea[aria-invalid=true]:active,textarea[aria-invalid=true]:focus{--border-color:var(--form-element-invalid-active-border-color)!important}input::-webkit-input-placeholder,input::placeholder,select:invalid,textarea::-webkit-input-placeholder,textarea::placeholder{color:var(--form-element-placeholder-color);opacity:1}input:not([type=checkbox]):not([type=radio]),select,textarea{margin-bottom:var(--spacing)}select::-ms-expand{border:0;background-color:transparent}select:not([multiple]):not([size]){padding-right:calc(var(--form-element-spacing-horizontal) + 1.5rem);background-image:var(--icon-chevron);background-position:center right .75rem;background-repeat:no-repeat;background-size:1rem auto}input+small,select+small,textarea+small{display:block;width:100%;margin-top:calc(var(--spacing) * -0.75);margin-bottom:var(--spacing);color:var(--muted-color)}label>input,label>select,label>textarea{margin-top:calc(var(--spacing) / 4)}[type=checkbox],[type=radio]{-webkit-appearance:none;-moz-appearance:none;appearance:none;display:inline-block;width:1.25em;height:1.25em;margin-top:-.125em;margin-right:.375em;border-width:var(--border-width);vertical-align:middle;cursor:pointer}[type=checkbox]::-ms-check,[type=radio]::-ms-check{display:none}[type=checkbox]:checked,[type=checkbox]:checked:active,[type=checkbox]:checked:focus,[type=radio]:checked,[type=radio]:checked:active,[type=radio]:checked:focus{--background-color:var(--primary);--border-color:var(--primary);background-image:var(--icon-checkbox);background-position:center;background-repeat:no-repeat;background-size:.75em auto}[type=checkbox]~label,[type=radio]~label{display:inline-block;margin-right:.375em;margin-bottom:0;cursor:pointer}[type=checkbox]:indeterminate{--background-color:var(--primary);--border-color:var(--primary);background-image:var(--icon-minus);background-position:center;background-repeat:no-repeat;background-size:.75em auto}[type=radio]{border-radius:50%}[type=radio]:checked,[type=radio]:checked:active,[type=radio]:checked:focus{--background-color:var(--primary-inverse);border-width:.35em;background-image:none}[type=checkbox][role=switch]{--background-color:var(--switch-background-color);--border-color:var(--switch-background-color);--color:var(--switch-color);width:2.25em;height:1.25em;border:var(--border-width) solid var(--border-color);border-radius:1.25em;background-color:var(--background-color);line-height:1.25em}[type=checkbox][role=switch]:focus{--background-color:var(--switch-background-color);--border-color:var(--switch-background-color)}[type=checkbox][role=switch]:checked{--background-color:var(--switch-checked-background-color);--border-color:var(--switch-checked-background-color)}[type=checkbox][role=switch]:before{display:block;width:calc(1.25em - (var(--border-width) * 2));height:100%;border-radius:50%;background-color:var(--color);content:'';transition:margin 0.1s ease-in-out}[type=checkbox][role=switch]:checked{background-image:none}[type=checkbox][role=switch]:checked:before{margin-right:0;margin-left:calc(1.125em - var(--border-width))}[type=color]::-webkit-color-swatch-wrapper{padding:0}[type=color]::-moz-focus-inner{padding:0}[type=color]::-webkit-color-swatch{border:none;border-radius:calc(var(--border-radius) / 2)}[type=color]::-moz-color-swatch{border:none;border-radius:calc(var(--border-radius) / 2)}[type=date],[type=datetime-local],[type=month],[type=time],[type=week]{background-image:var(--icon-date);background-position:center right .75rem;background-repeat:no-repeat;background-size:1rem auto}[type=date]::-webkit-calendar-picker-indicator,[type=datetime-local]::-webkit-calendar-picker-indicator,[type=month]::-webkit-calendar-picker-indicator,[type=time]::-webkit-calendar-picker-indicator,[type=week]::-webkit-calendar-picker-indicator{opacity:0}[type=time]{background-image:var(--icon-time)}[type=file]{--color:var(--muted-color);padding:calc(var(--form-element-spacing-vertical)/2) 0;border:none;border-radius:0;background:none}[type=file]::-webkit-file-upload-button{--background-color:var(--secondary);--border-color:var(--secondary);--color:var(--secondary-inverse);margin-right:calc(var(--spacing) / 2);padding:calc(var(--form-element-spacing-vertical) / 2) calc(var(--form-element-spacing-horizontal) / 2);border:var(--border-width) solid var(--border-color);border-radius:var(--border-radius);outline:none;background-color:var(--background-color);box-shadow:var(--box-shadow);color:var(--color);font-size:1rem;font-weight:var(--font-weight);line-height:var(--line-height);text-align:center;cursor:pointer;transition:background-color var(--transition),border-color var(--transition),color var(--transition),box-shadow var(--transition)}[type=file]:active,[type=file]:focus,[type=file]:hover{--color:var(--color);border:none;background:none}[type=file]:active::-webkit-file-upload-button,[type=file]:focus::-webkit-file-upload-button,[type=file]:hover::-webkit-file-upload-button{--background-color:var(--secondary-hover);--border-color:var(--secondary-hover)}[type=range]{-webkit-appearance:none;-moz-appearance:none;appearance:none;display:block;width:100%;height:1.25rem;background:transparent}[type=range]::-webkit-slider-runnable-track{width:100%;height:0.25rem;border-radius:var(--border-radius);background-color:var(--range-border-color);transition:background-color var(--transition),box-shadow var(--transition)}[type=range]::-moz-range-track{width:100%;height:0.25rem;border-radius:var(--border-radius);background-color:var(--range-border-color);transition:background-color var(--transition),box-shadow var(--transition)}[type=range]::-ms-track{width:100%;height:0.25rem;border-radius:var(--border-radius);background-color:var(--range-border-color);transition:background-color var(--transition),box-shadow var(--transition)}[type=range]::-ms-fill-lower{background-color:var(--border-radius)}[type=range]::-webkit-slider-thumb{-webkit-appearance:none;width:1.25rem;height:1.25rem;margin-top:-0.5rem;border:2px solid var(--range-thumb-border-color);border-radius:50%;background-color:var(--range-thumb-color);cursor:pointer;transition:background-color var(--transition),transform var(--transition)}[type=range]::-moz-range-thumb{-webkit-appearance:none;width:1.25rem;height:1.25rem;margin-top:-0.5rem;border:2px solid var(--range-thumb-border-color);border-radius:50%;background-color:var(--range-thumb-color);cursor:pointer;transition:background-color var(--transition),transform var(--transition)}[type=range]::-ms-thumb{-webkit-appearance:none;width:1.25rem;height:1.25rem;margin-top:-0.5rem;border:2px solid var(--range-thumb-border-color);border-radius:50%;background-color:var(--range-thumb-color);cursor:pointer;transition:background-color var(--transition),transform var(--transition)}[type=range]:focus,[type=range]:hover{--range-border-color:var(--range-active-border-color);--range-thumb-color:var(--range-thumb-hover-color)}[type=range]:active{--range-thumb-color:var(--range-thumb-active-color)}[type=range]:active::-webkit-slider-thumb{transform:scale(1.25)}[type=range]:active::-moz-range-thumb{transform:scale(1.25)}[type=range]:active::-ms-thumb{transform:scale(1.25)}[type=search]{border-radius:5rem;padding-left:calc(var(--form-element-spacing-horizontal) + 1.75rem)!important;background-image:var(--icon-search);background-position:center left 1.125rem;background-repeat:no-repeat;background-size:1rem auto}[type=search]::-webkit-search-cancel-button{-webkit-appearance:none;display:none}table{width:100%;border-color:inherit;border-collapse:collapse;border-spacing:0;text-indent:0}td,th{padding:calc(var(--spacing) / 2) var(--spacing);border-bottom:var(--border-width) solid var(--table-border-color);color:var(--color);font-size:var(--font-size);font-weight:var(--font-weight);text-align:left}tr{background-color:var(--background-color)}table[role=grid] tbody tr:nth-child(odd){--background-color:var(--table-row-stripped-background-color)}code,kbd,pre,samp{font-family:var(--font-family);font-size:.875rem}pre{-ms-overflow-style:scrollbar;overflow:auto}code,kbd,pre{background:var(--code-background-color);color:var(--code-color);font-weight:var(--font-weight);line-height:initial}code,kbd{display:inline-block;padding:.375rem .5rem;border-radius:var(--border-radius)}pre{display:block;margin-bottom:var(--spacing);padding:var(--spacing);overflow-x:auto;background:var(--code-background-color)}pre>code{display:block;padding:0;background:transparent;font-size:14px;line-height:var(--line-height)}code b{color:var(--code-tag-color);font-weight:var(--font-weight)}code i{color:var(--code-property-color);font-style:normal}code u{color:var(--code-value-color);text-decoration:none}code em{color:var(--code-comment-color);font-style:normal}kbd{background-color:var(--code-kbd-background-color);color:var(--code-kbd-color);vertical-align:middle}hr{box-sizing:content-box;height:0;overflow:visible;border:none;border-top:1px solid var(--muted-border-color)}[hidden],template{display:none}dialog{display:block;position:absolute;right:0;left:0;width:-moz-fit-content;width:-webkit-fit-content;width:fit-content;height:-moz-fit-content;height:-webkit-fit-content;height:fit-content;margin:auto;padding:1em;border:solid;background-color:white;color:black}dialog:not([open]){display:none}canvas{display:inline-block}details{display:block;margin-bottom:var(--spacing);padding-bottom:calc(var(--spacing) / 2);border-bottom:var(--border-width) solid var(--accordion-border-color)}details summary{color:var(--accordion-close-summary-color);line-height:1rem;list-style-type:none;list-style-type:none;cursor:pointer;transition:color var(--transition)}details summary::-webkit-details-marker{display:none}details summary::marker{display:none}details summary::-moz-list-bullet{list-style-type:none}details summary:after{display:inline-block;width:1rem;height:1rem;float:right;transform:rotate(-90deg);background-image:var(--icon-chevron);background-position:center;background-repeat:no-repeat;background-size:1rem auto;content:'';transition:transform var(--transition)}details summary:focus{outline:none;color:var(--accordion-active-summary-color)}details summary~*{margin-top:calc(var(--spacing) / 2)}details summary~*~*{margin-top:0}details[open]>summary{margin-bottom:calc(var(--spacing) / 4)}details[open]>summary:not(:focus){color:var(--accordion-open-summary-color)}details[open]>summary:after{transform:rotate(0)}article{margin:var(--block-spacing-vertical) 0;padding:var(--block-spacing-vertical) var(--block-spacing-horizontal);overflow:hidden;border-radius:var(--border-radius);background:var(--card-background-color);box-shadow:var(--card-box-shadow)}article>footer,article>header,article>pre{margin-right:calc(var(--block-spacing-horizontal) * -1);margin-left:calc(var(--block-spacing-horizontal) * -1);padding:calc(var(--block-spacing-vertical) / 1.5) var(--block-spacing-horizontal);background-color:var(--card-sectionning-background-color)}article>header{margin-top:calc(var(--block-spacing-vertical) * -1);margin-bottom:var(--block-spacing-vertical);border-bottom:var(--border-width) solid var(--card-border-color)}article>footer,article>pre{margin-top:var(--block-spacing-vertical);margin-bottom:calc(var(--block-spacing-vertical) * -1);border-top:var(--border-width) solid var(--card-border-color)}nav,nav ul{display:flex}nav{justify-content:space-between}nav ol,nav ul{align-items:center;margin-bottom:0;padding:0;list-style:none}nav ol:first-of-type,nav ul:first-of-type{margin-left:calc(var(--spacing) * -0.5)}nav ol:last-of-type,nav ul:last-of-type{margin-right:calc(var(--spacing) * -0.5)}nav li{display:inline-block;margin:0;padding:var(--spacing) calc(var(--spacing) / 2)}nav li>*,nav li>input:not([type=checkbox]):not([type=radio]){margin-bottom:0}nav a{display:block;margin:calc(var(--spacing) * -1) calc(var(--spacing) * -0.5);padding:var(--spacing) calc(var(--spacing) / 2);border-radius:var(--border-radius);text-decoration:none}nav a:active,nav a:focus,nav a:hover{text-decoration:none}aside li,aside nav,aside ol,aside ul{display:block}aside li{padding:calc(var(--spacing) / 2)}aside li a{margin:calc(var(--spacing) * -0.5);padding:calc(var(--spacing) / 2)}progress{display:inline-block;vertical-align:baseline}progress{-webkit-appearance:none;-moz-appearance:none;appearance:none;display:inline-block;width:100%;height:.5rem;margin-bottom:calc(var(--spacing) / 2);overflow:hidden;border:0;border-radius:var(--border-radius);background-color:var(--progress-background-color);color:var(--progress-color)}progress::-webkit-progress-bar{border-radius:var(--border-radius);background:transparent}progress[value]::-webkit-progress-value{background-color:var(--progress-color)}progress::-moz-progress-bar{background-color:var(--progress-color)}@media (prefers-reduced-motion:no-preference){progress:indeterminate{background:var(--progress-background-color) linear-gradient(to right,var(--progress-color) 30%,var(--progress-background-color) 30%) top left/150% 150% no-repeat;animation:progressIndeterminate 1s linear infinite}progress:indeterminate[value]::-webkit-progress-value{background-color:transparent}progress:indeterminate::-moz-progress-bar{background-color:transparent}}@keyframes progressIndeterminate{0%{background-position:200% 0}to{background-position:-200% 0}}[aria-busy=true]{cursor:progress}[aria-busy=true]:not(input):not(select):not(textarea):before{display:inline-block;width:1em;height:1em;border:0.1875em solid currentColor;border-radius:1em;border-right-color:transparent;vertical-align:text-bottom;vertical-align:-.125em;animation:spinner 0.75s linear infinite;content:'';opacity:var(--loading-spinner-opacity)}[aria-busy=true]:not(input):not(select):not(textarea):not(:empty):before{margin-right:calc(var(--spacing) / 2)}[aria-busy=true]:not(input):not(select):not(textarea):empty{text-align:center}a[aria-busy=true],button[aria-busy=true],input[type=button][aria-busy=true],input[type=reset][aria-busy=true],input[type=submit][aria-busy=true]{pointer-events:none}@keyframes spinner{to{transform:rotate(360deg)}}[data-tooltip]{position:relative}[data-tooltip]:not(a):not(button):not(input){border-bottom:1px dotted;text-decoration:none;cursor:help}[data-tooltip]:after,[data-tooltip]:before{display:block;z-index:99;position:absolute;bottom:100%;left:50%;padding:.25rem .5rem;overflow:hidden;transform:translate(-50%,-0.25rem);border-radius:var(--border-radius);background:var(--tooltip-background-color);color:var(--tooltip-color);font-size:.875rem;font-style:normal;font-weight:var(--font-weight);text-decoration:none;text-overflow:ellipsis;white-space:nowrap;content:attr(data-tooltip);opacity:0;pointer-events:none}[data-tooltip]:after{padding:0;transform:translate(-50%,0rem);border-top:.3rem solid;border-right:.3rem solid transparent;border-left:.3rem solid transparent;border-radius:0;background-color:transparent;color:var(--tooltip-background-color);content:''}[data-tooltip]:focus:after,[data-tooltip]:focus:before,[data-tooltip]:hover:after,[data-tooltip]:hover:before{opacity:1;animation-name:slide;animation-duration:.2s}[data-tooltip]:focus:after,[data-tooltip]:hover:after{animation-name:slideCaret}@keyframes slide{0%{transform:translate(-50%,0.75rem);opacity:0}to{transform:translate(-50%,-0.25rem);opacity:1}}@keyframes slideCaret{0%{opacity:0}50%{transform:translate(-50%,-0.25rem);opacity:0}to{transform:translate(-50%,0rem);opacity:1}}[aria-controls]{cursor:pointer}[aria-disabled=true],[disabled]{cursor:not-allowed}[aria-hidden=false][hidden]{display:initial}[aria-hidden=false][hidden]:not(:focus){clip:rect(0,0,0,0);position:absolute}[tabindex],a,area,button,input,label,select,summary,textarea{-ms-touch-action:manipulation}@media (prefers-reduced-motion:reduce){:not([aria-busy=true]),:not([aria-busy=true]):after,:not([aria-busy=true]):before{background-attachment:initial!important;animation-duration:1ms!important;animation-delay:-1ms!important;animation-iteration-count:1!important;scroll-behavior:auto!important;transition-delay:0s!important;transition-duration:0s!important}}
    </style>
    <style type="text/css">
      article div {
        font-size: 1.15em;
        line-height: 1.6em;
      }
    </style>
  </head>
  <body style="background-color: #f9fafb;">
    <main class="container">
      <nav>
        <ul>
          <li><a href="../index.html">← Topic list</a></li>
        </ul>
      </nav>
      <!-- <h1>Infomedia Articles Selection: 50 articles</h1> -->
      <h1>hSBM 0_15 <kbd>H_0_15</kbd></h1>
      <h3>Words (top 25)</h3>
      <div style="margin:0px -12px">
        <span style="font-size:29.29pt; padding:0px 12px"><strong>billeder</strong>&nbsp;<span style="font-size:.5em">3347</span></span>
        <span style="font-size:22.18pt; padding:0px 12px"><strong>billedet</strong>&nbsp;<span style="font-size:.5em">1249</span></span>
        <span style="font-size:19.68pt; padding:0px 12px"><strong>kameraer</strong>&nbsp;<span style="font-size:.5em">754</span></span>
        <span style="font-size:19.29pt; padding:0px 12px"><strong>ansigt</strong>&nbsp;<span style="font-size:.5em">687</span></span>
        <span style="font-size:18.56pt; padding:0px 12px"><strong>indbygget</strong>&nbsp;<span style="font-size:.5em">571</span></span>
        <span style="font-size:17.47pt; padding:0px 12px"><strong>objekter</strong>&nbsp;<span style="font-size:.5em">419</span></span>
        <span style="font-size:17.43pt; padding:0px 12px"><strong>ansigter</strong>&nbsp;<span style="font-size:.5em">413</span></span>
        <span style="font-size:16.03pt; padding:0px 12px"><strong>optage</strong>&nbsp;<span style="font-size:.5em">253</span></span>
        <span style="font-size:15.39pt; padding:0px 12px"><strong>ansigtet</strong>&nbsp;<span style="font-size:.5em">193</span></span>
        <span style="font-size:15.23pt; padding:0px 12px"><strong>genkender</strong>&nbsp;<span style="font-size:.5em">179</span></span>
        <span style="font-size:15.18pt; padding:0px 12px"><strong>vinkler</strong>&nbsp;<span style="font-size:.5em">175</span></span>
        <span style="font-size:14.83pt; padding:0px 12px"><strong>kameraerne</strong>&nbsp;<span style="font-size:.5em">147</span></span>
        <span style="font-size:14.41pt; padding:0px 12px"><strong>optagelserne</strong>&nbsp;<span style="font-size:.5em">116</span></span>
        <span style="font-size:14.39pt; padding:0px 12px"><strong>finger</strong>&nbsp;<span style="font-size:.5em">115</span></span>
        <span style="font-size:14.03pt; padding:0px 12px"><strong>face</strong>&nbsp;<span style="font-size:.5em">92</span></span>
        <span style="font-size:13.86pt; padding:0px 12px"><strong>filme</strong>&nbsp;<span style="font-size:.5em">82</span></span>
        <span style="font-size:13.79pt; padding:0px 12px"><strong>telefonopkald</strong>&nbsp;<span style="font-size:.5em">78</span></span>
        <span style="font-size:13.70pt; padding:0px 12px"><strong>brillerne</strong>&nbsp;<span style="font-size:.5em">73</span></span>
        <span style="font-size:13.45pt; padding:0px 12px"><strong>videooptagelser</strong>&nbsp;<span style="font-size:.5em">60</span></span>
        <span style="font-size:13.35pt; padding:0px 12px"><strong>sporer</strong>&nbsp;<span style="font-size:.5em">55</span></span>
        <span style="font-size:13.26pt; padding:0px 12px"><strong>bearbejder</strong>&nbsp;<span style="font-size:.5em">51</span></span>
        <span style="font-size:13.21pt; padding:0px 12px"><strong>pc'en</strong>&nbsp;<span style="font-size:.5em">49</span></span>
        <span style="font-size:13.19pt; padding:0px 12px"><strong>håndholdt</strong>&nbsp;<span style="font-size:.5em">48</span></span>
        <span style="font-size:13.19pt; padding:0px 12px"><strong>filmer</strong>&nbsp;<span style="font-size:.5em">48</span></span>
        <span style="font-size:13.12pt; padding:0px 12px"><strong>kerner</strong>&nbsp;<span style="font-size:.5em">45</span></span>
      </div>
      <br><br>
      <h3>Articles (top 50)</h3>
      Query:
      <pre>SELECT * FROM articles WHERE articles.dupeKeep=1 AND articles.sizeKeep=1 ORDER BY H_0_15 DESC LIMIT 50</pre>
      <article>
        <h4>Ansigter kan gøres mindeværdige</h4>
        <div>
          Et computerprogram, der manipulerer ansigter på billeder mere mindeværdige.Det har forskere ved MIT i USA skabt. De har udregnet en algoritme, som manipulerer billeder, så de bliver mere eller mindre mindeværdige. Da forskerne viste de manipulerede billeder til 80 forsøgspersoner, viste det sig, at computerprogrammet havde formået at gøre ansigterne henholdsvis lettere og vanskeligere at huske i mere end syv ud af ti tilfælde.
        </div>
        <footer>
          <em>Jyllands-Posten</em>
          &nbsp;·&nbsp; 2014-02-23
          &nbsp;·&nbsp; e446994a
          &nbsp;·&nbsp; #0
          <br>
          <br>
            <kbd data-tooltip="AI &amp; new algorithmic solutions">L02_AIALGO&nbsp;0.868</kbd>
            <kbd data-tooltip="AI, progress &amp; cybersecurity concerns">L05_AISECUR&nbsp;0.821</kbd>
            <kbd data-tooltip="hSBM 2_0">H_2_0&nbsp;0.649</kbd>
        </footer>
      </article>
      <article>
        <h4>Kineserne bruger ansigtsgenkendelse til at betale med</h4>
        <div>
          Via en app kan kineserne betale med deres ansigt. 150 millioner kinesere er allerede i gang.Nu er teknologien til ansigtsgenkendelse så sikker, at kineserne kan overføre penge og betale med deres ansigt via Alibabas mobilbetalings-app Alipay.Det skriver Quartz.150 millioner kinesere har indtil videre benyttet sig af, at de kan bruge deres ansigt til at overføre penge.I Kina har regeringen gemt billeder af befolkningen i en database. Det gør, at ansigtsgenkendelse gennem AI kan og bliver brugt til mange ting.For eksempel kan man også benytte sig af at bruge sit ansigt som identifikation ved bycyklerne Didi, og den hos kinesiske Google, kaldet Baidu, kan man bruge sit ansigt som adgangskort og billet til for eksempel offentlig transport.Version2 har tidligere skrevet om, hvordan fastfood-kæden Kentucky Fried Chicken benytter sig af ansigtsgenkendelse til at tilbyde kunderne det, de plejer godt at kunne lide af mad.
        </div>
        <footer>
          <em>Version2.dk</em>
          &nbsp;·&nbsp; 2017-07-07
          &nbsp;·&nbsp; e653b264
          &nbsp;·&nbsp; #1
          <br>
          <br>
            <kbd data-tooltip="Tech industry &amp; regulation">L02_BIGTEC&nbsp;0.714</kbd>
            <kbd data-tooltip="Facebook, fake news &amp; Trump">L05_FBTRUM&nbsp;0.543</kbd>
            <kbd data-tooltip="hSBM 2_0">H_2_0&nbsp;0.624</kbd>
        </footer>
      </article>
      <article>
        <h4>Javascript genkender ansigter i browseren</h4>
        <div>
          Nu kan Javascript-udviklere bygge ansigtsgenkendelse i webapps. Face-api.js er et Javascript-api til ansigtsgenkendelse i browseren implementeret oven på Googles Tensorflow.js, som er en Javascript-implementering af firmaets populære machine learning teknologi til browsere og Node.js-miljøet.Til teknologibloggen Infoq forklarer Vincent Mühler, som er skaberen af Face-api.js motivationen bag:»Jeg havde et andet bibliotek, som var i stand til at detektere ansigter og udføre ansigtsgenkendelse med Node.js. På et tidspunkt opdagede jeg Tensorflow.js og blev interesseret i maskinlæring i browseren. Jeg var nysgerrig på, om det var muligt at flytte eksisterende modeller til ansigtsgenkendelse og ansigtsgenkendelse til Tensorflow.js, og det fungerede ganske godt.« Face-api.js kommer med tre modeller: SSD Mobilenet V1, Tiny Face Detector og MTCNN.Tiny Face Detector er trænet på et brugerdefineret datasæt med 14.000 billeder. Apps med begrænsede resurser bør bruge denne model.Til ansigtsgenkendelse benyttes en model baseret på en ResNet-34-lignende arkitektur, til at beregne en ansigtsbeskrivelse ud fra et billede. Denne model er ikke begrænset til det sæt ansigter, der bruges til træning, hvilket betyder, at udviklere kan bruge det til genkendelse af alle personernes ansigter. Det er muligt at bestemme ligheden mellem to vilkårlige ansigter, ved at sammenligne deres ansigtsbeskrivelser.
        </div>
        <footer>
          <em>Version2.dk</em>
          &nbsp;·&nbsp; 2018-11-14
          &nbsp;·&nbsp; e6f8ed73
          &nbsp;·&nbsp; #2
          <br>
          <br>
            <kbd data-tooltip="AI &amp; new algorithmic solutions">L02_AIALGO&nbsp;0.977</kbd>
            <kbd data-tooltip="AI, progress &amp; cybersecurity concerns">L05_AISECUR&nbsp;0.912</kbd>
            <kbd data-tooltip="hSBM 2_0">H_2_0&nbsp;0.665</kbd>
        </footer>
      </article>
      <article>
        <h4>Du behøver ikke at fortrække en mine: Dit ansigt afslører dig alligevel</h4>
        <div>
          Nyt studie viser, at farveskift i ansigtet afslører menneskets følelser.Små farveskift i ansigtet, som på billede to er forstærkede, kan afsløre dine følelser. Foto: Ohio State UniversityAnne FilbertSelvom du holder dit kølige blik, din mund i en lige steg og din pande glat og uden en rynke, så afslører dit ansigt dig alligevel. Et nyt studie har nemlig vist, at det ikke nødvendigvis er trækningerne i dit ansigt, der giver dig væk.Studiet undersøgte evnen til at genkende andres følelser alene ud fra små skift i farven omkring næsen, øjenbrynene, kinderne eller hagen. Ifølge forskningsholdet beviser studiet en sammenhæng mellem centralnervesystemet og følelsesmæssige udtryk i ansigtet.Farveskiftene i ansigtet kommer nemlig af en sammenhæng mellem centralnervesystemet og blodtilførslen i ansigtet, der ændrer sig i takt med vores følelser.Det har derfor også lykkedes forskere at konstruere et computerprogram, der kunne genkende menneskers følelser vha. ansigtfarve med en succesrate på 90 pct.Forskerholdet, der kommer fra Ohio State University, har valgt at tage patent på systemet, da de mener, at forskningen kan bruges til at udvikle kunstig intelligens og dermed lære maskiner at genkende og efterligne menneskelige følelser.For at undersøge om folk rent faktisk kunne genkende følelser alene ud fra farveskift i ansigtet, retoucherede forskerholdet en række billeder af ansigter uden udtryk, hvor de så puttede forstærkede farver på alt efter hvilken følelse, de gerne ville have dem til at vise.Ca. 75 pct. af tilfældende gættede testpersonerne rigtigt i og kunne derfor genkende følelser alene ud fra farvekskiftene. Et andet forsøg gik ud på at vende om på ansigtsudtryk og farveskift.Farverne i billedet er forstærket for at illustrere, hvordan blodet strømmer til forskellige steder i ansigtet, alt efter hvilken følelse man har. Foto: Ohio State UniversityHer tog forskerne f.eks. de farveforstærkere, der indikerer glæde, og satte dem på en person, der viste et nedtrykt ansigtsudtryk. Her kommenterede størstedelen af testpersonerne, at der var noget underligt ved ansigtet.Den følelse, der var nemmest for både computere og testpersoner at genkende, var glæde. Det blev tydeligt i forsøget, at glæde i højere grad fik blodet til at strømme til specifikke steder i ansigtet, hvilket gjorde følelsen lettere genkendelig.
        </div>
        <footer>
          <em>Jyllands-posten.dk</em>
          &nbsp;·&nbsp; 2018-03-21
          &nbsp;·&nbsp; e6ac75b1
          &nbsp;·&nbsp; #3
          <br>
          <br>
            <kbd data-tooltip="AI &amp; new algorithmic solutions">L02_AIALGO&nbsp;0.927</kbd>
            <kbd data-tooltip="Art, Sci-Fi and AI">L05_SCIFI&nbsp;0.606</kbd>
            <kbd data-tooltip="hSBM 2_0">H_2_0&nbsp;0.647</kbd>
        </footer>
      </article>
      <article>
        <h4>ER DU KLAR OVER, HVILKEN BIAS TEKNOLOGIEN SERVERER?</h4>
        <div>
          Tech-journalist Marie Høst undersøger teknologiens blinde vinkler i ingeniørforeningen, IDAs podcast ' Blinde Vinkler'.Her kigger hun ind i teknologier og videnskab, som forsøger at rette op på udfordringen med køns-og etnicitetsmæssig bias. De forskellige podcasts tager bl. a.fat i rekruttering via machine learning og stemmestyrede digitale tjenester.Blinde vinkler Lyt her: bit. ly/ techman1.
        </div>
        <footer>
          <em>Tech Management</em>
          &nbsp;·&nbsp; 2020-11-13
          &nbsp;·&nbsp; e7fd2243
          &nbsp;·&nbsp; #4
          <br>
          <br>
            <kbd data-tooltip="AI &amp; new algorithmic solutions">L02_AIALGO&nbsp;0.592</kbd>
            <kbd data-tooltip="AI, progress &amp; cybersecurity concerns">L05_AISECUR&nbsp;0.542</kbd>
            <kbd data-tooltip="hSBM 2_0">H_2_0&nbsp;0.554</kbd>
        </footer>
      </article>
      <article>
        <h4>Spionkameraer mod elge</h4>
        <div>
          SVERIGE: Når elgene bevæger sig rundt i de svenske skove, skal de fremover vænne sig til, at mennesker kigger med.Der er nemlig ved at blive sat kameraer op i skovene i Gävleborg i Sverige som en del af et EU-projekt.De skal samle billeder ind af elgene, så dyrene på den måde fremover kan tælles uden hjælp fra jægere.Kameraerne bliver aktiveret af varme og bevægelse. Ved hjælp af kunstig intelligens skal det så være muligt at genkende elgene.
        </div>
        <footer>
          <em>Ekstra Bladet</em>
          &nbsp;·&nbsp; 2021-01-04
          &nbsp;·&nbsp; e80ebeee
          &nbsp;·&nbsp; #5
          <br>
          <br>
            <kbd data-tooltip="AI &amp; new algorithmic solutions">L02_AIALGO&nbsp;0.847</kbd>
            <kbd data-tooltip="AI, progress &amp; cybersecurity concerns">L05_AISECUR&nbsp;0.589</kbd>
            <kbd data-tooltip="hSBM 2_0">H_2_0&nbsp;0.706</kbd>
        </footer>
      </article>
      <article>
        <h4>Facebook-algoritme genkender dig selvom dit ansigt er skjult</h4>
        <div>
          Det sociale medie fortsætter sine bestræbelser inden for ansigtsgenkendelse og har nu udviklet en algoritme, som kan identificere en afbildet person med 83 procents nøjagtighed, selvom ansigtet ikke er synligt.Af Martin Bernth Tirsdag, 23. juni 2015 - 15:10Det seneste nye fra Facebooks laboratorium for kunstig intelligens er en -ansigtsgenkendende- algoritme, som ikke behøver dit ansigt for at identificere en person på billedet.Det skriver New Scientist.I stedet for at analysere ansigter sammenholder den nye algoritme andre genkendelige træk såsom frisure, beklædning, kropsform og -holdning og skal en genkendelsesrate på 83 procent.Algoritmen er udviklet med baggrund i 40.000 billeder fra tjenesten Flickr, hvor ansigter både var synlige og skjulte og fodret ind i et -sofistikeret neuralt netværk-, uden at dette beskrive nærmere.Umiddelbart virker algoritmen oplagt til at lægge på som et ekstra søgefilter i Facebooks netop lancerede tjeneste, Moments, (i USA, red. ), som gennemsøger en brugers telefon for billeder af venners ansigter, som systemet dernæst bundler, og tilbyder at dele privat med vennerne.Den nye algoritme er dog også et tveægget sværd i debatten om privacy, da den på den ene side er et lovende detektionsværktøj for personer, der ønsker at blive notificeret, når et billede af dem dukker op på nettet.På den anden side, kan man argumentere for, at algoritmen vil skabe panderyner hos privacy-fortalere, der vil problematiser, at det nu er muligt at blive identificeret på billeder, man end ikke er klar over, man indgår i.Via: New Scientist
        </div>
        <footer>
          <em>Version2.dk</em>
          &nbsp;·&nbsp; 2015-06-23
          &nbsp;·&nbsp; e517e44c
          &nbsp;·&nbsp; #6
          <br>
          <br>
            <kbd data-tooltip="AI &amp; new algorithmic solutions">L02_AIALGO&nbsp;0.506</kbd>
            <kbd data-tooltip="AI, progress &amp; cybersecurity concerns">L05_AISECUR&nbsp;0.581</kbd>
            <kbd data-tooltip="hSBM 2_0">H_2_0&nbsp;0.663</kbd>
        </footer>
      </article>
      <article>
        <h4>Ny algoritme fra Adobe: Fjerner linselus fra dit feriefoto</h4>
        <div>
          Fremover er det slut med at få uønskede ansigter og mennesker med på dine feriebilleder. Adobe har udviklet en ny feature, der kan fjerne både bevægelige og stationære objekter fra dine billeder. Algoritmen blev præsenteret på Max Konferencen i denne uge.eAdobe rigtig vil ikke rigtig forklare, hvordan den nye funktion - kaldet Monument Mode - fungerer, men det ser det ud til, at algoritmen både kan registrere stationær objekter (Eiffeltårnet, for eksempel) og bevægelige objekter (turister) i realtid, og fjerne dem fra dit fotoskud.Før og efter. (Adobe)Her demonstrerer skuespiller Nick Offerman og Adobe community manager Kim Chambers funktionen (i fem utroligt akavede minutter).Adobe benytter ofte sine konferencer til at fremvise nye teknologier, som kan integreres i deres produkter. Idet Monument Mode fjerner objekter fra et foto, før det er taget, er det sandsynligvis til en mobil app. Det vides endnu ikke hvornår og om det nogensinde bliver tilgængelige for forbrugere.Indtil da må vi nøjes med Photoshop.
        </div>
        <footer>
          <em>Version2.dk</em>
          &nbsp;·&nbsp; 2015-10-09
          &nbsp;·&nbsp; e5412820
          &nbsp;·&nbsp; #7
          <br>
          <br>
            <kbd data-tooltip="AI &amp; new algorithmic solutions">L02_AIALGO&nbsp;0.754</kbd>
            <kbd data-tooltip="AI, progress &amp; cybersecurity concerns">L05_AISECUR&nbsp;0.821</kbd>
            <kbd data-tooltip="hSBM 2_0">H_2_0&nbsp;0.596</kbd>
        </footer>
      </article>
      <article>
        <h4>Ritzau Plus: Overvågning i Mannheim</h4>
        <div>
          MannheimMannheim Way 2.0:* Der opsættes 71 videokameraer. * Kameraerne opsættes på 28 offentlige steder, hvor der ofte begås kriminalitet.* Krypterede optagelser sendes til en computer på byens politistation.* Et computerprogram udviklet på Fraunhofer Institute i Karlsruhe analyserer bevægelsesmønstre fra optagelserne ved hjælp af en algoritme. * Finder computeren et atypisk mønster, tilkaldes en politibetjent. * Alle billeder slettes efter tre dage.Kilde: Borgmesterkontoret i Mannheim/ritzau/Denne nyhed må publiceres digitalt bag paywall fra d. 17/02/2018 14:00Denne nyhed publiceres ikke på NET-tjenesten
        </div>
        <footer>
          <em>Ritzaus Bureau</em>
          &nbsp;·&nbsp; 2018-02-17
          &nbsp;·&nbsp; e69f9590
          &nbsp;·&nbsp; #8
          <br>
          <br>
            <kbd data-tooltip="AI &amp; new algorithmic solutions">L02_AIALGO&nbsp;0.681</kbd>
            <kbd data-tooltip="AI, progress &amp; cybersecurity concerns">L05_AISECUR&nbsp;0.575</kbd>
        </footer>
      </article>
      <article>
        <h4>Du behøver ikke at fortrække en mine: Dit ansigt afslører dig alligevel</h4>
        <div>
          Nyt studie viser, at farveskift i ansigtet afslører menneskets følelser.Selvom du holder dit kølige blik, din mund i en lige steg og din pande glat og uden en rynke, så afslører dit ansigt dig alligevel. Et nyt studie har nemlig vist, at det ikke nødvendigvis er trækningerne i dit ansigt, der giver dig væk.Studiet undersøgte evnen til at genkende andres følelser alene ud fra små skift i farven omkring næsen, øjenbrynene, kinderne eller hagen. Ifølge forskningsholdet beviser studiet en sammenhæng mellem centralnervesystemet og følelsesmæssige udtryk i ansigtet.Farveskiftene i ansigtet kommer nemlig af en sammenhæng mellem centralnervesystemet og blodtilførslen i ansigtet, der ændrer sig i takt med vores følelser.Det har derfor også lykkedes forskere at konstruere et computerprogram, der kunne genkende menneskers følelser vha. ansigtfarve med en succesrate på 90 pct.Forskerholdet, der kommer fra Ohio State University, har valgt at tage patent på systemet, da de mener, at forskningen kan bruges til at udvikle kunstig intelligens og dermed lære maskiner at genkende og efterligne menneskelige følelser.For at undersøge om folk rent faktisk kunne genkende følelser alene ud fra farveskift i ansigtet, retoucherede forskerholdet en række billeder af ansigter uden udtryk, hvor de så puttede forstærkede farver på alt efter hvilken følelse, de gerne ville have dem til at vise.Ca. 75 pct. af tilfældende gættede testpersonerne rigtigt i og kunne derfor genkende følelser alene ud fra farvekskiftene. Et andet forsøg gik ud på at vende om på ansigtsudtryk og farveskift.Her tog forskerne f.eks. de farveforstærkere, der indikerer glæde, og satte dem på en person, der viste et nedtrykt ansigtsudtryk. Her kommenterede størstedelen af testpersonerne, at der var noget underligt ved ansigtet.Den følelse, der var nemmest for både computere og testpersoner at genkende, var glæde. Det blev tydeligt i forsøget, at glæde i højere grad fik blodet til at strømme til specifikke steder i ansigtet, hvilket gjorde følelsen lettere genkendelig.
        </div>
        <footer>
          <em>Jyllands-posten.dk (Abonnementsområde)</em>
          &nbsp;·&nbsp; 2018-03-21
          &nbsp;·&nbsp; e6ac75be
          &nbsp;·&nbsp; #9
          <br>
          <br>
            <kbd data-tooltip="AI &amp; new algorithmic solutions">L02_AIALGO&nbsp;0.914</kbd>
            <kbd data-tooltip="Art, Sci-Fi and AI">L05_SCIFI&nbsp;0.642</kbd>
            <kbd data-tooltip="hSBM 2_0">H_2_0&nbsp;0.65</kbd>
        </footer>
      </article>
      <article>
        <h4>Nvidia-AI genererer troværdige billeder af opdigtede mennesker</h4>
        <div>
          Ved at fodre en AI med 30.000 billeder af kendisser, er den i stand til at genere troværdige billeder af, hvordan den mener, mennesker ser ud. Og den gør det godt.Nvidia har udviklet en algoritme, som er i stand til at skabe billeder af mennesker udelukkende baseret på en viden om, hvordan mennesker generelt ser ud.Det skriver Quartz.Nvidia GAN, som systemet hedder, er egentlig i sig selv simpelt. Man viser to AI's omkring 30.000 billeder af kendte mennesker ansigter, og fortæller, at det er sådan, mennesker ser ud. Derefter bliver den ene AI sat til at skabe troværdige billeder af mennesker, mens den anden hele tiden fortæller, hvorvidt billedet faktisk er troværdigt. Her fortsætter animeringen frem til, at der er bygget et troværdigt billede, og de to AI's går videre til næste design.Antallet er nøglenFaktisk formår AI'en at skabe billeder, som for det meste ikke falder i'the uncanny valley', eller som ser særligt unaturlige ud. Og her er nøglen antallet. I 2012 viste en undersøgelse, at mængden af data, som man sender gennem et neuralt netværk er alfa-omega i forhold til succesen af den opgave, netværket skal udføre.Og i dette tilfælde fodrede Nvidia som sagt AI'en med 30.000 billeder af, hvordan kendte mennesker så ud, og det er derfra, at præcisionen kommer. Dette skal naturligvis lægges til erfaringerne, som AI'en løbende drager.Ifølge Quartz er systemet allerede ved at gøre sit indtog i modeverdenen, hvor den, grundet den høje grad af realisme, kan øjnes at erstatte modefotograferne.
        </div>
        <footer>
          <em>Version2.dk</em>
          &nbsp;·&nbsp; 2017-11-01
          &nbsp;·&nbsp; e67b8dc4
          &nbsp;·&nbsp; #10
          <br>
          <br>
            <kbd data-tooltip="AI &amp; new algorithmic solutions">L02_AIALGO&nbsp;0.991</kbd>
            <kbd data-tooltip="AI, progress &amp; cybersecurity concerns">L05_AISECUR&nbsp;0.815</kbd>
            <kbd data-tooltip="hSBM 2_0">H_2_0&nbsp;0.713</kbd>
        </footer>
      </article>
      <article>
        <h4>Googles jpg-dræber: WebP viser billedet før det er hentet</h4>
        <div>
          Se billederne: Google vil skubbe det traditionelle billedformat JPEG ud i grøften med open source-billedformat WebP, som giver brugeren mulighed for at se billedet, før det er fuldt downloadet.Af Anne Lykke ukomprimeret foto i PNG-formatGoogle forsøger nu at mane det traditionelle billedformat JPEG i jorden med det nye billedformat WebP. WebP giver lossy komprimering af fotografier, men fylder 39,8 procent mindre end Jpeg-billeder af samme kvalitet.Læs også: Google vil slanke billed-formater: Klar med afløser for JPEGDerudover gør en helt ny feature, at WebP-billeder trinvist kan dekodes, mens computeren downloader billedet fra websiden, hvilket gør browseren i stand til at vise billedet, før hele filen faktisk er downloaded. Denne feature virker allerede i Chrome 12. Det skriver Googles produktchef Richard Rabbat og softwareudvikler Pascal Massimino på Chromium blog.Yderligere har WebP også fået integreret en 'fancy upsampler', som reducerer pixeleringen af stærke kanter. Som det ses på billedet af klodserne, er kanterne i billedet med 'fancy upsampling' mindre pixelerede end uden 'fancy upsampling', hvor der forekommer trappeagtige konturer.På kodnings-siden har Google fokuseret på at forbedre kvaliteten ved at indføre et filter, der kan inddele billedet i ens områder, som komprimeres på samme måde. Mens nogle dele af billedet let kan komprimeres meget, uden at det går ud over kvaliteten, vil andre områder kræve flere bits. Disse kaldes svære områder, som man kan se på det sidste billede.Derved kan WebP bevare mange af detaljerne fra det oprindelige billede i modsætning til Jpeg, hvor der kan forekomme såkaldte omringende elementer i billedet. Disse omringende elementer kan være forstyrrende farver eller prikker, der danner en ring om billedet ved for dårlig komprimering.WebP er bklandt andet sat sammen af algoritmerne bag videoformatet VP8, som Google har gjort gratis at bruge under navnet WebM.
        </div>
        <footer>
          <em>Version2.dk</em>
          &nbsp;·&nbsp; 2011-05-23
          &nbsp;·&nbsp; e2b18abf
          &nbsp;·&nbsp; #11
          <br>
          <br>
            <kbd data-tooltip="AI &amp; new algorithmic solutions">L02_AIALGO&nbsp;0.809</kbd>
            <kbd data-tooltip="AI, progress &amp; cybersecurity concerns">L05_AISECUR&nbsp;0.796</kbd>
            <kbd data-tooltip="hSBM 2_0">H_2_0&nbsp;0.685</kbd>
        </footer>
      </article>
      <article>
        <h4>IBM bruger uvidende menneskers ansigter til at udvikle ansigtsgenkendelse</h4>
        <div>
          IBM har fodret algoritmer til ansigtsgenkendelse med næsten en million Flickr-billeder uden at spørge menneskerne på billederne om lov. Det afslører NBC i en lang udredning.IBM har indsamlet billederne fra et større datasæt på 99,2 million billeder, som er delt under en Creative Commons-licens, hvilket normalt giver rettigheder til at billederne kan blive brugt frit.Men det er ikke sikkert, at rettighederne går så vidt til, at de afbillede skal finde sig i, at deres ansigter bliver brugt til at udvikle ansigtskendelsesalgoritmer.Relateret jobannonce: Commercial Analyst to optimise wind farm business casesDen ansvarlige for AI-forskning hos IBM, John Smith, siger til NBC, at enhver har lov til at bede om at få en specifik URL fjernet fra datasættet.Men det kræver, at fotografen bag det specifikke billeder sender linket til billedet, og det kan blive svært, fordi IBM ikke har offentliggjort listen over, hvilke billeder der indgår i datasættet.
        </div>
        <footer>
          <em>Version2.dk</em>
          &nbsp;·&nbsp; 2019-03-13
          &nbsp;·&nbsp; e71d546e
          &nbsp;·&nbsp; #12
          <br>
          <br>
            <kbd data-tooltip="AI &amp; new algorithmic solutions">L02_AIALGO&nbsp;0.975</kbd>
            <kbd data-tooltip="AI, progress &amp; cybersecurity concerns">L05_AISECUR&nbsp;0.876</kbd>
            <kbd data-tooltip="hSBM 2_0">H_2_0&nbsp;0.638</kbd>
        </footer>
      </article>
      <article>
        <h4>Apple udgiver første videnskabelige artikel om AI billedgenkendelse</h4>
        <div>
          Et hold af seks forskere løfter i artiklen sløret for Apples machine learning-aspirationer.For at træne computere til billedgenkendelse sender Apple neurale netværk i krig mod hinanden. Ideen hedder Adversarial Training og er ikke helt nyt, men Apples machine learning hold har tilpasset deres GANs på en ny måde. Det skriver Engadget.Det er langt lettere at bruge computergenererede billeder til AI-udvikling i stedet for almindelige billeder, der først skal -tagges- manuelt af ansatte. Dette sætter forskere i et dilemma, da resultatet med de syntetiske billeder ikke bliver af samme kvalitet.Løsningen er ifølge Apple at lade et andet system forsøge at adskille syntetiske fra ægte billeder for dermed at trimme de algortimer, der producerer de syntetiske billeder.Apple har længe holdt AI kortene tæt til kroppen, men kan nu ligesom resten af tech giganterne (Facebook, Google, Microsoft, m.fl.) bryste sig af at deltage i den akademiske machine learning debat.
        </div>
        <footer>
          <em>Version2.dk</em>
          &nbsp;·&nbsp; 2016-12-27
          &nbsp;·&nbsp; e60b3a07
          &nbsp;·&nbsp; #13
          <br>
          <br>
            <kbd data-tooltip="AI &amp; new algorithmic solutions">L02_AIALGO&nbsp;0.76</kbd>
            <kbd data-tooltip="AI, progress &amp; cybersecurity concerns">L05_AISECUR&nbsp;0.711</kbd>
            <kbd data-tooltip="hSBM 2_0">H_2_0&nbsp;0.634</kbd>
        </footer>
      </article>
      <article>
        <h4>Tekstiler med &quot;ansigtsprint- skjuler dig for kameraerne&quot;</h4>
        <div>
          Med Hyperface vil kunstnere skabe et tekstiltryk, der kan snyde algoritmer til at se et utal af ansigter og dermed holde bæreren af stoffet uidentificeret. Foto: Adam HarveyVed at bære stof med et mønster af, hvad computeralgoritmer anser for ansigter i massevis, kan bæreren slippe udenom ansigtsgenkendelse.Af Liv Bjerg LillevangMed projektet Hyperface vil kunstneren Adam Harvey tage kampen op mod Facebook og reklamevirksomheders algoritmer til ansigtsgenkendelse og dermed sikre privatliv på internettet.Hyperface er et stoftryk, der i en computeralgoritmes optik bliver et virvar af øjne, næser og munde. Med nøje udvalgte sammensætninger af pixels i forskellige størrelser, skal stoffet således snyde algoritmen til at se ansigter overalt på billedet, hvor tekstilet er synligt.På den måde er det Adam Harveys ønske, at bærere af Hyperface-tekstiler kan sikre sig anonymitet på internetbilleder. Det skriver The Guardian, til hvem privacy-kunsteren fortæller, at klædet skal »overlæsse en algoritme med det, den søger, ved at mætte et område med ansigter og dermed aflede computeralgoritmens blik«.
        </div>
        <footer>
          <em>Ing.dk (Ingeniøren)</em>
          &nbsp;·&nbsp; 2017-01-04
          &nbsp;·&nbsp; e60db403
          &nbsp;·&nbsp; #14
          <br>
          <br>
            <kbd data-tooltip="AI &amp; new algorithmic solutions">L02_AIALGO&nbsp;0.72</kbd>
            <kbd data-tooltip="AI, progress &amp; cybersecurity concerns">L05_AISECUR&nbsp;0.531</kbd>
            <kbd data-tooltip="hSBM 2_0">H_2_0&nbsp;0.636</kbd>
        </footer>
      </article>
      <article>
        <h4>Digitalt: Ryd op i dine fotos med kunstig intelligens</h4>
        <div>
          Du behøver ikke stresse over alle de billeder, der fylder lageret op på din smartphone eller computer. Ny teknologi kan hjælpe dig med at få dem sorteret og arkiveret.Vi tager billeder som aldrig før. Analysefirmaet InfoTrends vurderer, at der i 2017 blev taget 1.200 milliarder digitale billeder, eller cirka 160 billeder for hver af klodens 7,5 mia. Indbyggere. Tænk lige over det. Denne statistik viser, at vi mennesker, fra den fattigste tigger til den rigeste milliardær, fra det nyfødte spædbarn til den ældste beboer på plejehjemmet, i gennemsnit tager et billede cirka hver anden dag.Sådan er virkeligheden naturligvis ikke. Verdens digitale fotos er langt mere ulige fordelt. Det ved du naturligvis allerede, for når du kigger på din smartphone eller på din harddisk, ligger der så mange billeder, at du har svært ved at overskue, hvordan du skal få dem organiseret og sat i orden.Heldigvis findes der nu smarte værktøjer, der blandt andet via kunstig intelligens kan hjælpe dig med lige præcis den opgave - og på den platform, du foretrækker.Det er naturligvis kameraerne i vores lommer, der har sat ild i den digitale billedeksplosion, og for mange mennesker kommer billederne aldrig videre. Hvis de skal vises til andre, kommer telefonen bare op fra lommen. Hvis du helst vil bruge smartphonen - og kun smartphonen - som fotoplatform, kan du overveje disse gratis apps.Zyl. Appen bruger metadata som tidspunkt og geografisk lokation for fotografiet til at anbefale, hvordan du kan organisere billederne i mapper, der f.eks. kan deles med venner. Men du kan også lade være, og det gør Zyl til et godt bud på en app til at rydde op i dine billeder, hvis du ikke har lyst til at lægge dine billeder op i skyen eller på sociale medier af privatlivshensyn.Ever. Hvis du derimod ikke er helt så bekymret for at lægge dine billeder i skyen, kan du overveje Ever. Denne app organiserer ikke bare smartphone-fotos, men også billeder fra sociale medier som Facebook og Instagram. Det betyder, at du også kan få billeder, andre har taget af dig, ind i samlingen. Ever er gratis at bruge, hvis du kan nøjes med komprimerede billeder, men fuld opløsning kræver betalingsversionen af appen.Slidebox. Måske har du helt styr på dine fotomapper selv og har bare behov for et værktøj, der hjælpe dig med hurtigt at slette eller arkivere billederne på din telefon? Så skal du have fat i Slidebox. Med denne app kan du stryge til venstre eller til højre, afhængig af om du vil gemme eller slette billedet, og du kan lynhurtigt placere de gemte billeder i de mapper, du har lavet.Billeder på computerenDigitale billeder kommer dog ikke altid fra din smartphone. De kan komme fra et rigtigt digitalkamera eller på en e-mail fra et familiemedlem. Derfor kan en computer ofte være et bedre sted at organisere sine billeder. Den større skærm afslører flere fejl, og der er også mere harddiskplads at gemme billederne på.Windows Photos. I efteråret opdaterede Microsoft sin Billeder-app til Windows 10, så du nu kan fritekstsøge efter indhold i fotos. Hvis du for eksempel skriver -øl- eller -Skagen-, dukker der billeder op fra det nordjyske, eller fotos med øl ombord. Den kunstige intelligens hjælper dig også med at lave fotoalbums eller videoer på en nem måde og kan sortere dine billeder efter de personer, der er på dem.Apple Photos. Ligesom på Windows-pc-er er der nu også kunstig intelligens og søgning indbygget i den medfølgende Fotos-app på Mac, der hjælper dig med at sortere billeder i kategorier som -mine minder- og -personer-. Men Fotos har også relativt avanceret billedredigering modsat mange af konkurrenterne.Adobe Photoshop Elements. Denne let skrabede version af det professionelle Photoshop-program har ud over avancerede redigeringsfunktioner også objekt-genkendelse via kunstig intelligens indbygget. Det betyder, at programmet kan lave såkaldte -smart tags-, der er beskrivelser af, hvad der er på billedet, som du så kan søge på og sortere efter senere.I skyenMange sky-lagertjenester såsom Dropbox understøtter nu automatisk upload af billeder, når du har taget dem på smartphonen. Oftest kan du dog bare bruge de sky-tjenester, der følger med, når du køber noget ny teknologi, som f.eks. disse:Google Photos. Googles fototjeneste følger naturligvis med Android-telefoner, men kan også bruges på f.eks. iPhone. Google har en af de meste avancerede fotogenkendelsesteknologier i skyen, der helt automatisk kan forbedre dine billeder, lave albums og animationer, hvis du ikke har lyst til at bruge tid på det og ikke er nervøs over, at Googles servere kigger i dine billeder.iCloud Photo Stream. Hvis du slår iCloud Photo Stream til under indstillinger på din iPhone eller iPad, bliver dine billeder automatisk lagt op på det iCloud-lager i skyen, der fulgte med dit Apple-produkt.Herfra kan du sortere på samme måde som i Fotos-appen på Mac.Microsoft OneDrive. Har du en Windows-pc eller en Surface-tablet, medfølger der også et OneDrive-lager i skyen, som du kan uploade dine billeder til. Også her kan du få hjælp fra en kunstig intelligens til at sortere dine billeder, og du kan søge i dem og få adgang til dem hvor som helst. Hvis du downloader OneDrive-appen til iOS eller Android, kan du også uploade nye smartphone-billeder automatisk.
        </div>
        <footer>
          <em>Politiken.dk</em>
          &nbsp;·&nbsp; 2018-02-18
          &nbsp;·&nbsp; e69fdd5f
          &nbsp;·&nbsp; #15
          <br>
          <br>
            <kbd data-tooltip="AI &amp; new algorithmic solutions">L02_AIALGO&nbsp;0.681</kbd>
            <kbd data-tooltip="AI, progress &amp; cybersecurity concerns">L05_AISECUR&nbsp;0.75</kbd>
            <kbd data-tooltip="hSBM 2_0">H_2_0&nbsp;0.624</kbd>
        </footer>
      </article>
      <article>
        <h4>Nu kan vi alle blive Google Street View-fotografer</h4>
        <div>
          Google har annonceret flere nye funktioner i Google Street View, hvor du kan se 360-graders billeder fra gader og områder i hele verden. Hidtil har Google selv stået for gadefotograferingen, mens fotografer efterhånden har kunnet bidrage med punktbilleder fra steder og virksomheder.Nu slipper Google også publikum løs på selve gadebillederne. Hvis du som fotograf har taget bedre eller nyere 360-graders billeder af et gadeområde, vil det være dine billeder, der vises Street View.Algoritmerne styrer»Vores algoritmer analyserer billedkvaliteten, og tidspunktet de er taget på, og vurderer på den måde, om de skal erstatte vores egne billeder i et bestemt område. Der lægges specielt vægt på, om billederne er nyere end de eksisterende,« forklarer Program Manager Amit Moraya fra Google Maps til Teknisk Ukeblad.Google har forhåndsgodkendt en række 360-graders kameraer til opgaven, idet de har de tekniske specifikationer til at klare opgaven. Blandt disse kameraer er Ricoh Theta V, Insta360 One og Insta360 Pro.Google bruger også avancerede algoritmer til at genkende steder og skilte på de nye billeder, som de allerede gør med Street View-billeder nu.Understøtter videoNu understøttes også 360-graders video, som bruges til at hente billedsekvenser over et bestemt område. Dette dukker op som en stiplet linje på kortene, som man kan navigere efter.Foreløbig understøttes denne funktion kun af mobiltelefoner eller andre enheder med Androids operativsystem. Der stilles også krav til, at fotograferne har en vis status som bidragydere i systemet.Google understreger, at fotograferne fortsat har rettigheder til billederne, som bruges til Street View og kan fjerne dem igen når som helst. Fotografer får også mulighed for at lægge deres navn eller logo i bunden af billedet, når man ser lige ned.»Jeg synes, det er en spændende mulighed for lokale myndigheder og turistorganisationer, som kan sørge for opdaterede og gode billeder af deres egne områder,« siger Amit Moraya til TU.Ved hjælp af en tidslinjefunktion er det muligt at bladre tilbage til tidligere versioner af gadebillederne i et område, både fra Google selv og andre bidragydere.Du kan se de danske byer, Google snart planlægger at køre igennem, her.Denne artikel er fra digi.no.
        </div>
        <footer>
          <em>Version2.dk</em>
          &nbsp;·&nbsp; 2018-05-31
          &nbsp;·&nbsp; e6c5a18f
          &nbsp;·&nbsp; #16
          <br>
          <br>
            <kbd data-tooltip="AI &amp; new algorithmic solutions">L02_AIALGO&nbsp;0.61</kbd>
            <kbd data-tooltip="AI, progress &amp; cybersecurity concerns">L05_AISECUR&nbsp;0.669</kbd>
            <kbd data-tooltip="hSBM 2_0">H_2_0&nbsp;0.664</kbd>
        </footer>
      </article>
      <article>
        <h4>Sådan jagter det norske politi ID-svindlere og andre kriminelle med ansigtsgenkendelse</h4>
        <div>
          Det norske politi er begyndt at anvende ansigtsgenkendelse til brug for tjek af pas og id-kort, samt i indvandringssager og straffesager. Det sker i Kripos, som er det norske politis kompetencecenter for biometri.Det fortæller Version2s norske søstermedie, Digi.no, i en artikel om politiets erfaringer med ansigtsgenkendelse i Norge. Etaten arbejder med relativt nyt it-system med automatisert ansigtsgenkendelse, ABIS (Automated Biometric Information System).Det norske politi har hjemmel til at optage fingeraftryk og billeder, fx under en anholdelse eller i en forhørssituation, hvor en borger er under mistanke. Med den nye ansigtsgenkendelsesteknologi kan ansigtsfotoet sammenlignes med politiets eksisterende billedregister ved hjælp af en algoritme.Algoritmerne svarer med en liste med cirka 25 hits, hvor sammenfaldet er størst, hvorefter det gennemgås manuelt af to sagsbehandlere.»Man bliver ikke dømt i Norge, fordi ansigtsgenkendelsesystemet giver et hit, der er ugunstigt for dig. I disse tilfælde skal eksperterne vurdere billederne og foretage en sammenligning af ansigtet,« siger Frøy Løvåsdal, der er seniorrådgiver i politidirektoratet til digi.no.Med andre ord kan teknologien være et vigtigt element, men den bliver står ikke alene.»I lande med dårlig retssikkerhed, kan teknologien klart være en trussel,« tilføjer Frøy Løvåsdal.Skal fange falske pasTeknologien anvendes også til 1:1 sammenligning med billeder, som tages i forbindelse med, at man søger om nyt pas. Det nye pasbillede sammenlignes med det, som blev taget sidste gang en borger søgte om pas eller ID-kort.Desuden vil man gennemføre 1:N - en til mange - søgninger. Det vil sige, at et nyt billede sammenlignes med billeder af andre pas/ID-kort indehavere, som er registreret i databasen for at sikre at et ansigt ikke registreres på en andens navn. Begge metoder skal gøre det vanskeligere at få udstedt et norsk pas og ID-kort med falsk identitet.Eksperter foretager manuel kontrolEn udfordring ved ansigtsbiometri er at - i modsætning til fingeraftryk - ændrer ansigtet sig i løbet af livet, og desuden er billedkvaliteten afgørende.Relateret jobannonce: ProjektlederI Norge forsøger man at løse dette problem med et eksperthold, der skal evaluere tvivlsspørgsmål ved at sammenligne billeder manuelt, i form af såkaldt morfologisk analyse. Seks fagfolk har fået uddannelse fra internationale eksperter i blandt andet anatomi, billedanalyse og kognitiv psykologi.»Vi er ikke fingeraftryks-eksperter, selvom vi er født med ti fingre. Og selvom mange måske tror det, er vi heller ikke så gode til at sammenligne ansigter. Derfor er det vigtig at styrke vores kompetence,« siger Frøy Løvåsdal fra politidirektoratet.Blandt de vigtige kompetencer er at man ikke må stole stole blindt på algoritmer.Algoritmerne bliver bedre, men de kan ikke erstatte mennesker, siger Berit Lima, Kripos Forensic Department Section Manager.Private overvågningsbilleder skal også indgåPolitiets ABIS-system får snart ny funktionalitet som kan kode billeder fra private overvågningskameraer. Disse billeder kan dermed også sammenkøres med politiets billedregister eller op imod andre overvågningsbilleder. På denne måde vil databasen vokse og blive mere funktionel over tid, fortæller politiet.Berit Lima i Kripos understreger, at overvågningsbilleder kun vil blive kodet og anvendt i forbindelse med straffesager og op imod overvågningsbilleder og politiets billedregister.Samtidig understreger man, at billederne ikke vil blive delt med tredjeparter, ejheller med andre lande eller andre aktører.Lufthavnen i Oslo, Gardermoen, er et eksempel på en lokation, hvor der er etableret en anden form for biometrisk kontol. Lufthavnen har automatiseret grænsekontrol, hvor de rejsendes ansigter sammenlignes med billedet i passet.
        </div>
        <footer>
          <em>Version2.dk</em>
          &nbsp;·&nbsp; 2019-05-14
          &nbsp;·&nbsp; e732947d
          &nbsp;·&nbsp; #17
          <br>
          <br>
            <kbd data-tooltip="AI &amp; new algorithmic solutions">L02_AIALGO&nbsp;0.752</kbd>
            <kbd data-tooltip="AI, progress &amp; cybersecurity concerns">L05_AISECUR&nbsp;0.818</kbd>
            <kbd data-tooltip="hSBM 2_0">H_2_0&nbsp;0.667</kbd>
        </footer>
      </article>
      <article>
        <h4>Twitter undskylder for race-bias i algoritme til beskæring af billeder</h4>
        <div>
          Flere bruger har påvist, hvordan det sociale medie Twitter har tilbøjelighed til at fokusere på hvide ansigter frem for sorte, når billeder skal beskæres. Twitter har undskyldt for at have race-bias i den algoritme, som bruges til billedbeskæring på det sociale medie. Det skriver The Guardian Undskyldningen kommer i kølvandet på, at flere brugere har påvist, hvordan billeder på det sociale medie har tilbøjelighed til at fokusere på hvide ansigter frem for sorte, når billeder bliver beskåret automatisk. Ifølge Twitter er algoritmen blevet testet for bias, inden den blev taget i brug, men nu har det sociale medie måtte erkende, at det ikke er blevet gjort tilstrækkeligt grundigt. Når brugere uploader store eller lange billeder til Twitter, har algoritmen sørget for, at billederne blev beskåret, så de ikke fyldte så meget i feedet på det sociale medie. Algoritmen er udviklet til at fokusere på de væsentligste dele af billedet og sortere resten fra. Men i weekenden opdagede flere brugere, at algoritmen ofte sorterer sorte ansigter fra frem for hvide, når det skal beskære billeder til at passe til feedet. En talsperson for Twitter har i en udtalelse erkendt, at man skal tilbage til tegnebrættet med algoritmen. »Vores team har testet for bias, inden vi tog modellen i brug, og der fandt vi ikke tegn på race- eller kønsbias. Men ud fra disse eksempler (fra brugerne, red.) står det klart, at vi skal lave flere analyser. Vi vil fortsætte med at dele, hvad vi lærer, og hvilke beslutninger vi tager, og vi vil dele vores analyse, så andre kan gennemgå og efterligne den,«  lød det ifølge The Guardian i udtalelsen fra Twitter. Se et af brugernes eksempler nedenfor. Flere eksempler kan findes her
        </div>
        <footer>
          <em>Version2.dk</em>
          &nbsp;·&nbsp; 2020-09-22
          &nbsp;·&nbsp; e7e711bc
          &nbsp;·&nbsp; #18
          <br>
          <br>
            <kbd data-tooltip="Tech industry &amp; regulation">L02_BIGTEC&nbsp;0.675</kbd>
            <kbd data-tooltip="Facebook, fake news &amp; Trump">L05_FBTRUM&nbsp;0.614</kbd>
            <kbd data-tooltip="Social media &amp; well being">L10_SOMEWB&nbsp;0.611</kbd>
            <kbd data-tooltip="hSBM 2_0">H_2_0&nbsp;0.649</kbd>
        </footer>
      </article>
      <article>
        <h4>AI kan reparere og gøre slørede billeder skarpe</h4>
        <div>
          Uden at benytte andre billeder som inspiration, kan helisk neuralt netværk gøre billeder bedre.Et dybt helisk neuralt netværk kan gøre uskarpe billeder blive skarpe, ved kun at bruge information fra selve billedet. Effekten er dog mindre synlig her på grund af Version2's CMS-system. Photo: Dmitry UlyanovDet er fup og fidus, men det virker: Kunstig intelligens kan genskabe billedinformation, så resultatet til forveksling ligner originalen.Teknologien har været fremme i et stykke tid, men nu har tre russiske forskere skabt en algoritme, der siger spar to til tidligere forsøg. Det er de såkaldte 'dybe heliske neurale netværk'(deep convolutional neural networks, CNN) som er velegnede til opgaver af denne type. Det skriver Bleeping Computer.Deep Image Prior, som algoritmen er navngivet, adskiller sig fra andre tilsvarende algoritmer, ved at benytte billedets egen information til reparationsopgaverne, i stedet for at bygge på et træningssæt med andre billeder til udbedring af fejl og mangler.Det kan blandt andet benyttes til at gøre slørede billeder skarpe, til at fjerne støj af forskellig art og til føje manglende dele af et billede.Algoritmen EnhanceNet-PAT kan skabe et billede (nr. 3 fra venstre) ud fra en sløret original (nr. 1), så det til forveksling ligner originalen (th). Photo: Mehdi S. M. SajjadiKoden bag er skrevet i Python og kan hentes hos Github.Deep Image Prior er ikke den eneste nye algoritme, der kan genskabe informationer i billeder, så det næppe er til at se. En anden ny algoritme, EnhanceNet-PAT, benytter i modsætning til Deep Image Prior et testsæt til at gætte på den manglende information. Teknologien kan blandt andet anvendes til at skabe billeder med højere opløsning end originalen.
        </div>
        <footer>
          <em>Version2.dk</em>
          &nbsp;·&nbsp; 2018-01-03
          &nbsp;·&nbsp; e6904d2d
          &nbsp;·&nbsp; #19
          <br>
          <br>
            <kbd data-tooltip="AI &amp; new algorithmic solutions">L02_AIALGO&nbsp;0.987</kbd>
            <kbd data-tooltip="AI, progress &amp; cybersecurity concerns">L05_AISECUR&nbsp;0.986</kbd>
            <kbd data-tooltip="hSBM 2_0">H_2_0&nbsp;0.61</kbd>
        </footer>
      </article>
      <article>
        <h4>Forskere skaber endnu mere livagtige billeder af kunstige ansigter</h4>
        <div>
          Forskere fra Nvidia har i en videnskabelig artikel vist eksempler på meget realistiske ansigter fremstillet med brug af kunstig intelligens, skriver PetaPixel.Artiklen (PDF) indeholder billeder fra resultater af deres løsning, og de er meget vanskelige at skelne fra den ægte vare.Forskerne kalder løsningen Generative Adversarial Network (GAN), og den indebærer kort fortalt, at man danner kunstige billeder med to neurale netværk, hvor man forsøger at &quot;narre&quot; det ene med kunstige billeder.Netværket er fodret med store datasæt af ægte billeder og skal forsøge at skille de ægte fra falske.Teknologien er langt fra ny, og allerede i november sidste år fortalte digi.no, hvordan GAN-teknologien blev brugt til at skabe ekstremt realistiske ansigter.Også dengang stod Nvidia-forskere bag.Nvidia-forskerne har imidlertid forbedret teknologien ved at gøre det muligt for systemet automatisk at skelne mellem forskellige &quot;niveauer&quot; af attributter, for eksempel mellem positurer og såkaldte stokastiske variabler, som er mere vilkårlige detaljer som fregner og hår.Derudover er datasættet nu meget større med 70.000 højkvalitetsbilleder med ekstra stor variation.Ved sidste års demonstration blev der til sammenligning kun anvendt 30.000 billeder af kendte mennesker.Den nye generator har også den fordel, at man kan justere kombinationen af forskellige attributkategorier individuelt og dermed &quot;redigere&quot; de genererede billeder. Det giver mulighed for en større kontrol med resultatet.Hvad teknologien præcist er beregnet til er usikkert, men som The Verge påpegede i forbindelse med sagen, kan den ikke kun anvendes til positive formål.Propaganda og reduceret tillid til billedmateriale som dokumentation kan være blandt de negative konsekvenser, ligesom der kan skabes falske identiteter.Alle tekniske detaljer findes i forskningsdokumentet.Artiklen stammer fra digi.no.
        </div>
        <footer>
          <em>Version2.dk</em>
          &nbsp;·&nbsp; 2018-12-28
          &nbsp;·&nbsp; e7066096
          &nbsp;·&nbsp; #20
          <br>
          <br>
            <kbd data-tooltip="AI &amp; new algorithmic solutions">L02_AIALGO&nbsp;0.882</kbd>
            <kbd data-tooltip="AI, progress &amp; cybersecurity concerns">L05_AISECUR&nbsp;0.888</kbd>
            <kbd data-tooltip="hSBM 2_0">H_2_0&nbsp;0.676</kbd>
        </footer>
      </article>
      <article>
        <h4>Ny AI-bot tegner overraskende detaljeret efter tekst</h4>
        <div>
          Microsoft har løftet sløret for deres nye AI-bot der kan tegne billeder alene ud fra tekstbeskrivelser.En fugl med sorte vinger, en gul krop og et lille næb. Det er de eneste informationer som Microsofts research lab har fodret deres nye AI-tegnebot, før den har tegnet en forholdsvis avanceret tegning med mange detaljer.Microsoft kalder deres nye AI-program for Drawing bot, og de har i et løftet sløret for teknologien bag i et paper der er udgivet ved Cornell University i USA.Den nye teknologi, der er drevet af kunstig intelligens og er fortsat under udvikling, ser på individuelle ord, når der skabes billeder fra tekst.Programmet kan generere billeder af alt fra klassiske scener med græssende dyr, til mere absurde situationer med en flyvende dobbeltdækkerbus. Hvert billede indeholder detaljer der ikke indgår i den tekst som tegneprogrammet har fået leveret.»Hvis du går på Bing og søger efter en fugl, så får du et fuglebillede. Men her er billedet skabt af computeren, pixel for pixel helt fra bunden. De her fugle findes ikke i den virkelige verden. De er blot en del af computerens forestilling om fugle,« siger Xiadong He, forskningsleder ved Deep Learning Technology Center hos Microsoft-s research lab i Redmond, Washington i et blogindlæg hos Microsoft.To modeller kæmperTegneprogrammet kommer i forlængelse af et udviklingsarbejde mellem computer vision og sprogprocessering som Microsoft har arbejdet med i en del år.Tidligere har udviklingsholdet lavet CaptionBot, der automatisk skriver billedtekster til billeder, ligesom de har udviklet AI-modeller der besvarer spørgsmål om konkrete billeder, fx lokation, objekter, hvilket kan være brugbart for eksempelvis blinde.Kernen i Microsofts tegne-bot er Generative Adversarial Network(GAN). Det er et netværk der består af to machine learning modeller, en der generer billeder fra tekstbeskrivelser og en anden der kaldes diskriminatoren, der bruger tekstbeskrivelser til at vurderer autenticiteten af de genererede billeder.Den genererings-modellen forsøger at få falske billeder forbi diskrimatoren, som omvendt forsøger at presse genereringsmodellen til at lave så optimale billeder som muligt.Tegne-programmet er trænet med datasæt der indeholder billeder og tekster der er parret, så modellen kan lære at matche ord med den visuelle repræsentation af ordene.GAN-netværket fungerer fint når den skal generere billeder fra simple tekstbeskrivelser, fx blå fugl, eller et grønt træ, men kvaliteten dalen i takt med at kompleksiteten stiger, da det er hele sætningen, der fungerer som input til GAN-netværket.Så når man skriver en fulg med en grøn krone, gule vinger og en rød mave, så bliver kvaliteten ikke så god, da de detaljerede informationer går tabt i beskrivelsen og billedet bliver eksempelvis mere uskarpt.Skal hjælpe filmproducenterTegneprogrammet forsøger at efterligne den menneskelige måde at tegne på ved at dele ordene op i forskellige afsnit af billedet.Det kaldes attentional GAN, eller AttnGAN, som matematisk repræsenterer det menneskelige koncept opmærksomhed.»Opmærksomhed er en menneskelig koncept og vi bruger så matematik til at gøre opmærksomhed til en beregning, « siger Xiandon He.P.t er teknologien endnu ikke funktionel, og når man ser tættere på billederne, vil man stort set hver gang se fejl.Ifølge Microsoft er AttnGAN-billeder dog alligevel tre gange bedre end de forrige GAN-netværk.På sigt håber Microsoft at deres tegne-bot kan bruges til at assistere malere eller hjælpe filmproducenter ved at tegne animerede scener baseret på et manuskript.
        </div>
        <footer>
          <em>Version2.dk</em>
          &nbsp;·&nbsp; 2018-01-19
          &nbsp;·&nbsp; e6957c40
          &nbsp;·&nbsp; #21
          <br>
          <br>
            <kbd data-tooltip="AI &amp; new algorithmic solutions">L02_AIALGO&nbsp;0.933</kbd>
            <kbd data-tooltip="AI, progress &amp; cybersecurity concerns">L05_AISECUR&nbsp;0.953</kbd>
            <kbd data-tooltip="hSBM 2_0">H_2_0&nbsp;0.663</kbd>
        </footer>
      </article>
      <article>
        <h4>Amazon giver chauffører et ultimatum: Lad jer overvåge, eller bliv fyret</h4>
        <div>
          Amazon vil i denne uge tvinge sine chauffører til at underskrive en blanket, hvor man giver samtykke til, at virksomheden må overvåge medarbejderne med AI-drevne kameraer i varevognene. Hvis ikke man siger 'ja', så bliver man fyret.Tech-giganten Amazon, har i denne uge givet sine chauffører et ultimatum: Enten siger man 'ja' til at lade sig overvåge, ellers mister man sit job.Det skriver ViceVirksomheden kræver, at chauffører i denne uge underskriver en ''biometrisk samtykke-blanket'', der giver Amazon lov til at bruge kameraer baseret på kunstig intelligens (AI) til at indsamle deres lokation, bevægelse og biometrisk data - blandt andet ansigtsgenkendelsesdata.I februar besluttede virksomheden at installere AI-drevne kameraer med fire linser i alle varevogne for at forbedre »sikkerheden«  og »kvaliteten af leveringsoplevelsen« . Kameraerne er udviklet af tech-firmaet Netradyne.Ifølge produktbeskrivelsen kan Netradynes kameraer blandt andet registrere, hvis en chauffør gaber, virker distraheret eller ikke har sikkerhedssele på.Deborah Bass, talsperson for Amazon, understreger, at kameraerne kun bruges til sikkerhedsformål:»Vi testede teknologien fra april til oktober 2020 på varevognsruter over to millioner mil, og resultaterne ledte til utrolige sikkerhedsforbedringer for chauffører og fællesskabet - ulykker gik 48 procent ned, færdselsovertrædelser med stopskilte gik 20 procent ned, chauffører uden sikkerhedssele faldt 60 procent, og distraheret kørsel faldt 45 procent. Tro ikke på egoistiske kritikere, som påstår, at kameraerne har noget som helst andet formål end sikkerhed,«  siger hun til Vice.I USA har Amazon omkring 75.000 chauffører, som leverer pakker.
        </div>
        <footer>
          <em>Version2.dk</em>
          &nbsp;·&nbsp; 2021-03-25
          &nbsp;·&nbsp; e82f348a
          &nbsp;·&nbsp; #22
          <br>
          <br>
            <kbd data-tooltip="Tech industry &amp; regulation">L02_BIGTEC&nbsp;0.679</kbd>
            <kbd data-tooltip="hSBM 2_0">H_2_0&nbsp;0.558</kbd>
        </footer>
      </article>
      <article>
        <h4>Nu er Facebook også for blinde</h4>
        <div>
          Facebook kan nu bruges af folk med synshandicap takket være en ny funktion i Facebooks iOS-app.Morten BayDengang, Facebook allermest handlede om at skrive sjove opdateringer, var det sociale netværk noget mere brugbart for blinde og andre med synshandicap.Men efterhånden som Facebook udviklede sig og teknologien tillod flere og flere billeder, blev folks nyheds-feeds i langt højere grad en visuel oplevelse.Fortæller, hvad der er i billedetFør kunne computeren blot læse op, hvad folk skrev i ens feed, og man kunne så skrive med et blindetastatur eller diktere til computeren, hvad man gerne ville svare. Men det er jo lidt sværere, når ens Facebook-billede siger mere end 1000 ord. Og kommentarer på billedet ovenfor giver ofte ikke ret meget mening, hvis man ikke har det visuelle med.Men det vil Facebook nu gøre noget ved. De har lanceret et nyt værktøj til deres Facebook-appen på iOS, der kan hjælpe svagtseende   i første omgang på engelsk. Det sker ved, at billedet bliver analyseret for objekter og disse objekter bliver så samlet på en liste, der så læses op.Facebook bruger kunstig intelligens-teknologi og neurale netværk til at identificere, hvad det er for objekter, der er i billedet, og er der en person til stede tydeligt i billedet, som hører til brugerens Facebook-vennekreds, bliver denne persons navn også nævnt.Facebooks computere lærer om billederI et eksempel fra Facebook er der et billede, der har kommentaren »Vi nåede frem!«. Den kunstige intelligens har så fundet ud af, at der formentlig er to mennesker, der smiler, solbriller, himmel og vand på billedet, og at det er taget udendørs.Således kan Facebook altså hjælpe svagtseende med at danne deres egne forestillinger af, hvad venner forsøger at formidle.Som med de fleste af denne slags teknologier finder Facebooks kunstige intelligens-software ud af, hvad der er på billedet ved at genkende det fra sin egen træning.Gennem det, man kalder maskinlæring fodres maskinen med informationer og billeder og dens indre logik processerer disse, sådan at softwaren kan genkende dem andre steder.Det er den samme teknologi, Google bruger til billedgenkendelse og billedsøgning, og eftersom flere og flere medier er visuelt orienterede, såsom Instagram, Snapchat, Vine med mere, giver denne teknologi brugere med synshandicap en chance for også at være med på sociale medier uden store begrænsninger.OPLÆSNING. Facebook kan nu sætte talte ord på de billeder, der bliver postet på Facebook. Det kan især hjælpe svagtseende og blinde, når de bruger sociale medier (Foto: Facebook)..OPLÆSNING. Facebook kan nu sætte talte ord på de billeder, der bliver postet på Facebook. Det kan især hjælpe svagtseende og blinde, når de bruger sociale medier (Foto: Facebook)..
        </div>
        <footer>
          <em>Politiken.dk</em>
          &nbsp;·&nbsp; 2016-04-07
          &nbsp;·&nbsp; e5a64ae2
          &nbsp;·&nbsp; #23
          <br>
          <br>
            <kbd data-tooltip="Tech industry &amp; regulation">L02_BIGTEC&nbsp;0.539</kbd>
            <kbd data-tooltip="Social media &amp; well being">L10_SOMEWB&nbsp;0.595</kbd>
            <kbd data-tooltip="hSBM 2_0">H_2_0&nbsp;0.654</kbd>
        </footer>
      </article>
      <article>
        <h4>Norsk politi har brugt kontroversiel ansigtsgenkendelses-app til efterforskning</h4>
        <div>
          Medarbejder hos norsk politi har anvendt appen med over tre milliarder billeder fra sociale medier til at efterforske i en sag om sex-overgreb mod børn. En medarbejder hos norsk politi har taget den kritiserede ansigtskendelses-app fra selskabet Clearview i brug til efterforskning. Appen, der har indsamlet mere end tre milliarder billeder fra sociale medier, kan bruges til at identificere fremmede ansigter. Det skriver NRK Medarbejderen hos det norske politi har taget appen i brug til efterforskning to uger efter, at både norsk og også dansk politi dukkede op på en kundeliste hos AI-selskabet Clearview. Dengang svarede begge myndigheder, at man hverken havde brugt teknologien eller var kunde hos Clearview.
        </div>
        <footer>
          <em>Version2.dk</em>
          &nbsp;·&nbsp; 2020-03-11
          &nbsp;·&nbsp; e79ddf21
          &nbsp;·&nbsp; #24
          <br>
          <br>
            <kbd data-tooltip="Tech industry &amp; regulation">L02_BIGTEC&nbsp;0.858</kbd>
            <kbd data-tooltip="Facebook, fake news &amp; Trump">L05_FBTRUM&nbsp;0.625</kbd>
            <kbd data-tooltip="hSBM 2_0">H_2_0&nbsp;0.535</kbd>
        </footer>
      </article>
      <article>
        <h4>Forskere rekonstruerer ansigter ved at aflæse hjerneaktiviteten</h4>
        <div>
          Amerikanske forskere mener at kunne genskabe ansigter alene ud fra andre menneskers tanker ved at se personen. En norsk forsker er dog skeptisk.Forskere fra New York University, Yale og Berkeley har gjort en både fascinerende og skræmmende opdagelse. Det skriver det norske tidsskrift Teknisk Ukeblad.Et studie, hvor seks personer blev vist billeder af ansigter, mens forskerne scannede deres hjerner med en såkaldt funktionel magnetresonanstomografi (fMRI), viste, at aflæsninger af hjernens aktivitet gav forskerne mulighed for at rekonstruere de ansigter, som forsøgspersonerne havde kigget på.Algoritme gør tankelæsning muligForsøget foregår i praksis ved, at forskerne viser forsøgspersonerne billeder af 300 forskellige ansigter. Ved hjælp fMRI dokumenterer de, hvordan hjernen bliver aktiveret af de forskellige ansigter.Dette datasæt overføres til en algoritme, der lærer, hvordan hver enkelt forsøgspersons hjerne reagerer på de komponenter, som ansigter har tilfælles. Derefter bliver et sæt nye ansigter fremvist for forsøgspersonerne, mens fMRI-optagelserne bliver foretaget.Herefter vil forskerne kunne identificere, hvilke nye ansigter der blev præsenteret for enkeltpersoner, alene ved at analysere hjernens aktivitet med algoritmen.»Vi genererer ansigtstræk matematisk, men ender med at få oplysninger om race, køn, ansigtsform med mere. Baseret på et billede af hjernens aktivitet kan vi forudsige nogenlunde, hvordan ansigtet ser ud,« siger Alan Cowen på UC Berkeley til Teknisk Ukeblad.Hjernens algoritme er individuelfMRI registrerer mængden af ilt, der forekommer i hver af hjernens voxels, der typisk indeholder over 100.000 neuroner. De oplysninger, forskerne er nødt til at bruge, er relativt grove. Det samme gælder derfor rekonstruktionerne.Ifølge Hallvard Røe Evensmoen, forsker ved Institut for Neurovidenskab ved Norges Teknisk-Naturvitenskapelige Universitet, er der samtidig meget stor forskel på den enkelte hjerne. Derfor skal der skabes en særlig algoritme, der er baseret på hjerneaktivitet fra hver enkelt forsøgsperson, hvis rekonstruktionen skal være korrekt, understreger han over for Teknisk Ukeblad.Alan Cowen fra Berkeley anerkender begge problemstillinger, men han tror, at det vil ændre sig i fremtiden.»Jeg tvivler ikke på, at vi før eller siden vil kunne læse hjernens aktivitet med tilstrækkelig nøjagtighed til at skabe genkendelige rekonstruktioner,« siger han til Teknisk Ukeblad.Et tydeligere øjenvidneAlan Cowen forklarer, at de neuroteknologiske metoder inden for ansigtsgenkendelse kan lære os meget om, hvordan hjernen opfatter ansigter. De kan blandt andet bruges til at forstå sygdomme som prosopagnosia (svært ved at genkende ansigter), autisme og skizofreni.»Måske en dag du kan bruge teknikken til at designe en slags virtuel virkelighed, som kan hjælpe disse mennesker i deres dagligdag. Samtidig kan teknikken anvendes til at undersøge erindringer, fantasier og drømme. Om 20 til 50 år har vi måske udviklet teknologien til et punkt, hvor man kan rekonstruere minder fra øjenvidner,« konstaterer forskeren over for Teknisk Ukeblad.Han anerkender dog, at den avancererede teknologi vil stille mennesket over for nogle etiske spørgsmål, da teknologien også vil kunne afsløre menneskets dybe begær, pinlige vaner, svagheder og frygt.Forsker Alan Cowen fra UC Berkeley tvivler ikke på, at man for fremtiden vil kunne videreudvikle teknologien, så man med nøjagtighed kan genskabe rekonstruktioner af ansigter og steder, som mennesker har været vidne til..Forsker Alan Cowen fra UC Berkeley tvivler ikke på, at man for fremtiden vil kunne videreudvikle teknologien, så man med nøjagtighed kan genskabe rekonstruktioner af ansigter og steder, som mennesker har været vidne til..
        </div>
        <footer>
          <em>Ing.dk (Ingeniøren)</em>
          &nbsp;·&nbsp; 2014-06-02
          &nbsp;·&nbsp; e46db09e
          &nbsp;·&nbsp; #25
          <br>
          <br>
            <kbd data-tooltip="AI &amp; new algorithmic solutions">L02_AIALGO&nbsp;0.993</kbd>
            <kbd data-tooltip="AI, progress &amp; cybersecurity concerns">L05_AISECUR&nbsp;0.849</kbd>
            <kbd data-tooltip="hSBM 2_0">H_2_0&nbsp;0.613</kbd>
        </footer>
      </article>
      <article>
        <h4>Kunstig intelligens gransker fotos fra 2. Verdenskrig</h4>
        <div>
          Moderne teknologi giver forskerne helt nye værktøjer. Takket være kunstig intelligens kan fotografier taget under 2. Verdenskrig blive kortlagt og kategoriseret nemmere end nogensinde før.Det skriver Aarhus Universitet i en pressemeddelelse ifølge Videnskab.dk.Ved hjælp af AI (kunstig intelligens) kan forskere ud fra indholdet på et billede nu genkende og navngive fotografier, som fotografer har taget.Læs mere på Videnskab.dk: 'Levende robotter' er skabt: Potentialet er kæmpestort - og dødsensfarligtDanske forskere fra AU Engineering ved Aarhus Universitet har samarbejdet med kollegaer fra Tampere-universitetet i Finland og det finske Miljøinstitut.Sammen har de gennemgået omkring 160.000 fotografier taget af i alt 23 finske fotografer fra Vinterkrigen, Fortsættelseskrigen og Laplandskrigen fra årene 1939-1945.Ved hjælp af AI'en kunne forskere automatisk registrere mennesker og objekter i forskellige scenarier langt hurtigere, end et menneske kunne have gjort det.Ifølge EU's tilgængelighedsdirektiv, der trådte i kraft i september 2020, skal alle offentlige billeder på internettet indeholde tekstbeskrivelser af billedindhold, og her kan AI-teknologien være til stor hjælp.Læs mere på Videnskab.dk: Tilfældigheder adskiller de stærkeste fra de svageste- Vi er ganske overraskede over den præcision, hvormed AI'en er i stand til at genkende fotografer ud fra kendetegn på billederne, såsom indhold og framing, siger Alexandros Losifidis, lektor og ekspert i kunstig intelligens ved Aarhus Universitet, i en pressemeddelelse ifølge Videnskab.dk.Gennemsnitligt opnåede AI'en en klassificeringsnøjagtighed på 41,1 procent.Studiet viser ifølge forskerne, at visse fotografer har en meget distinkt og letgenkendelig stil, mens det hos andre fotografer er sværere for AI'en at genkende.Andre artikler på Videnskab.dkNazisters buddhistiske figur stammer fra rummetSkal vi have sex med robotter?
        </div>
        <footer>
          <em>EkstraBladet.dk/Plus</em>
          &nbsp;·&nbsp; 2020-11-01
          &nbsp;·&nbsp; e7f7b72d
          &nbsp;·&nbsp; #26
          <br>
          <br>
            <kbd data-tooltip="AI &amp; new algorithmic solutions">L02_AIALGO&nbsp;0.883</kbd>
            <kbd data-tooltip="AI, progress &amp; cybersecurity concerns">L05_AISECUR&nbsp;0.565</kbd>
            <kbd data-tooltip="hSBM 2_0">H_2_0&nbsp;0.584</kbd>
        </footer>
      </article>
      <article>
        <h4>IBM har udviklet AI-malware, der venter med at angribe til det rette øjeblik</h4>
        <div>
          IBM har udviklet proof-of-concept malware baseret på et 'convolutional neural network', som kan forblive i ro indtil helt specifikke betingelser er opfyldtKonceptet hedder DeepLocker og bliver i dag præsenteret på sikkerhedskonferencen Black Hat USA, som bliver afholdt i Las Vegas.Det skriver The Register.Ved at bruge et krypteret payload og samtidig lade et neuralt netværk afgøre, hvornår det skal aktiveres, kan konceptet gøre det væsentligt sværere for sikkerhedsforskere og anti-virus-værktøjer at stoppe malwaren, skriver The Register.IBM har demonstreret konceptet ved at kryptere og gemme en kopi af Wannacry-ransomwaren i en videokommunikations-app sammen med kode, der bruger et neuralt netværk til at afgøre, hvornår en krypteringsnøgle skal frigives.Det neurale netværk, var blevet trænet til at vente indtil et bestemt ansigt blev opfanget på video-appen. Da den rette person satte sig foran PC'en genkendte koden ansigtet, nøglen blev frigivet, payload kørt og systemets dokumenter taget til gidsel.
        </div>
        <footer>
          <em>Version2.dk</em>
          &nbsp;·&nbsp; 2018-08-09
          &nbsp;·&nbsp; e6da5664
          &nbsp;·&nbsp; #27
          <br>
          <br>
            <kbd data-tooltip="AI &amp; new algorithmic solutions">L02_AIALGO&nbsp;0.815</kbd>
            <kbd data-tooltip="AI, progress &amp; cybersecurity concerns">L05_AISECUR&nbsp;0.639</kbd>
            <kbd data-tooltip="hSBM 2_0">H_2_0&nbsp;0.65</kbd>
        </footer>
      </article>
      <article>
        <h4>Barsk Pentax med unik teknologi</h4>
        <div>
          Ingen kameraer i denne prisklasse tåler så barsk behandling som Pentax K-3 spejlreflekskameraet med en helt ny funktion.Du har muligvis ikke tidligere hørt om en lavpasfiltersimulator? Det havde vi heller ikke - før nu. Men en sådan findes i Pentax K-3.De fleste kameraer har et filter foran billedsensoren, som hindrer, at der dannes moirémønstre i billederne. Noget, som ellers let kan ske, da sensorerne på overfladen af chip'en er grupperet i et mønster, som let kan give moiré, når man tager billeder af for eksempel tekstiler. Af samme grund er kameraer uden lavpasfiltre ikke populære hos modefotografer.LÆS OGSÅ: Kompromisløs 55 mm til fuldformatLavpasfiltret koster imidlertid detaljeskarphed. Derfor har man i Pentax K-3 droppet filtret, men har i stedet udstyret kameraet med en såkaldt lavpas-simulator.Læs testen af Pentax K-5 IIs uden lavpasfilter her .Simulatoren, som fungerer ved hjælp af den den bevægelige billedsensor, kan slås til og fra, og er den slået til, reducerer den risikoen for moiré-mønstre. Den rummer også et par andre tricks, så som at ryste støv af billedsensoren og lave automatisk niveaukontrol.Ikke nok med det, ved astrofotografering kan den flytte sig for at kompensere for jordrotationen ved lange lukketider. I så fald skal man dog investere i O-GPS-modulet som ekstraudstyr.Klar til det barske vejrDen bevægelige billedsensor kompenserer desuden for bevægelsesuskarphed på lange lukketider, hvilket betyder, at man kan skyde håndholdt på 1/15s og stadig have håb om at få skarpe billeder.Men kameraet har flere unikke egenskaber for prisklassen. Som en omfattende vejrforsegling. I K-3 er det hele 92 forseglinger som betyder, at kameraet skulle kunne fungere optimalt, selv når det øser ned, sner tæt, eller sandet pisker dig i ansigtet.Det har også en nyudviklet CMOS-sensor i APS-C størrelse med 24 MP, som sammen med den nye bildeprocessor Prime III, skal give minimal billedestøj ved høje ISO-værdier.K-3 har også et nyt autofokusmodul med 27 autofokuspunkter, hvoraf hele 25 er følsomme krydsensorer, som er placeret i centrum af søgeren, som fungerer i et område fra -3EV til +18EV.Det har også noget, som Pentax kalder &quot;Real-Time Analysis System&quot; som er en RGB lysmålingssensor med 86 000 pixels og en ny algoritme. Resultatet skulle være mere optimal lysmåling, og mere præcis autofokus, samt bedre hvidbalance.K-3 kan skyde sekvensbilleder med op til 8,3 billeder pr. sekund. Op til 22 billeder i serie i RAW-råformat, eller op til 60 billeder i serie i JPEG-format.Video i full-HDDen optiske søger er endda større end i K-5 og med bedre forstørrelse, og kameraet kan skyde video i full-HD 1080/60i/30p med H.264-komprimering og det kan filme intervaller med 4K-opløsning (3840 x 2160).LÆS OGSÅ: Nikon forbedrer sit drømmekameraEn mikrofonindgang i stereo findes også, samt en hovedetelefonudgang for monitoring hvor indspillingsniveauet fra den eksterne mikrofonen kan justeres direkte på kameraet.K-3 er ikke trådløst, men det kan brug en hukommelsekorttype kaldet Pentax Flu Card på 16GB lagringskapacitet, som giver adgang til at fjernstyre kameraet fra en smartphone, tage billeder, se live-billedet på telefonen, og lagre og overføre billeder.Pentax K-3 leveres i slutningen af november, og vil koste omkring 10.000 kr for kamerahuset, 10.500 kr med en 18-55mm zoom, eller ca. 13.000 kr med en vejrforseglet 18-135 mm zoom.En ny vejrforseglet zoom kan købes samtidig: HD Pentax-DA 55-300mm f4.0 vil koste ca. 3.500 kr.
        </div>
        <footer>
          <em>Jp.dk (Jyllands-Posten)</em>
          &nbsp;·&nbsp; 2013-10-15
          &nbsp;·&nbsp; e409b5c7
          &nbsp;·&nbsp; #28
          <br>
          <br>
            <kbd data-tooltip="AI &amp; new algorithmic solutions">L02_AIALGO&nbsp;0.701</kbd>
            <kbd data-tooltip="AI, progress &amp; cybersecurity concerns">L05_AISECUR&nbsp;0.779</kbd>
            <kbd data-tooltip="hSBM 2_0">H_2_0&nbsp;0.715</kbd>
        </footer>
      </article>
      <article>
        <h4>Video: Sådan kan man lave billedgenkendelse hos Baidu - uden kode</h4>
        <div>
          Nu er bølgen også nået til maskinlæring.Den kinesiske søgemaskinegigant Baidu har sendt sit bud på gaden under navnet EZDL (&quot;nem deep learning.&quot;) Det skriver ADTMag.EZDL anvender en fire-trins proces til projektudvikling og implementering af brugerdefinerede maskinlærings-modeller: Opret en model, upload og opmærk billeder eller andre ting, træn og test modellen og implementer resultatet med et cloud-API eller et offline udviklingsværktøj (SDK).Ifølge Baidu er værktøjet målrettet mod små og mellemstore virksomheder.»Selvom du ikke har haft nogen erfaring med programmering, kan du hurtigt bygge modeller på denne platform uden barrierer,« siger Yongkang Xie, som er teknologichef for Baidu EZDL.»EZDL kan hjælpe virksomheder med begrænset AI-ekspertise og it-ressourcer til hurtigt og effektivt at gennemføre træning med deep learning og implementering, selv med kun en lille mængde data.« Baidu fremhæver tre slags brugsscenarier:En model til automatisk klassificering af billeder med brugerdefinerede klasse til opgaver som: Klassificering af billeder af boliger, genkendelse af kinesisk urtemedicin, vilde fugle og frøer, samt industriel kvalitetskontrol i forbindelse med identifikation af defekte produkter.En model til automatisk detektering af objekter i billeder og optælling af antal objekter efter klasse, til opgaver som eksempelvis optælling af celler inden for medicin.En klassifikationsmodel til at genkende forskellige typer lydtyper eller detektere klasser af begivenheder, beregnet til opgaver som sikkerhedsovervågning og videnskabelig forskning.EZDL er en del af dets kunstig intelligensprojekt, der går under navnet Baidu Brain.På videoen herunder kan man se anvendelsen af EZDL.
        </div>
        <footer>
          <em>Version2.dk</em>
          &nbsp;·&nbsp; 2018-09-20
          &nbsp;·&nbsp; e6e7d057
          &nbsp;·&nbsp; #29
          <br>
          <br>
            <kbd data-tooltip="AI &amp; new algorithmic solutions">L02_AIALGO&nbsp;0.833</kbd>
            <kbd data-tooltip="AI, progress &amp; cybersecurity concerns">L05_AISECUR&nbsp;0.851</kbd>
            <kbd data-tooltip="hSBM 2_0">H_2_0&nbsp;0.614</kbd>
        </footer>
      </article>
      <article>
        <h4>Efter et årti bliver ikonisk datasæt privacy-bevidst: Nu er ansigterne forsvundet fra ImageNet</h4>
        <div>
          Det går hårdest ud over masker og mundharmonikaer. Forskere ved Stanford og Princeton har taget et signifikant skridt for at gøre ImageNet bedre egnet til udvikling af ansvarlig AI. I sidste uge blev det officielle datasæt opdateret til en version, hvor ansigter er blevet sløret.»Most categories in the ImageNet challenge are not people categories, however, many incidental people appear in the images, and their privacy is a concern,« skriver forskerne i en præpubliceret artikel om arbejdet.I datasættet, som hører til den årlige ImageNet-challenge - ImageNet Large Scale Visual Recognition Challenge (ILSVRC) - er der 1.000 kategorier, og kun tre af dem handler om mennesker.Ikke desto mindre optræder mennesker på tusindvis af datasættets billeder tagget med diverse objekter - personer, der drikker øl, sidder i stole, går tur med en hund etc.Mere specifikt fandt forskerne over en halv million ansigter på omkring en kvart million billeder - svarende til ansigter i 17 procent af de 1,4 millioner billeder i sættet.På grund af den måde, som billederne i sin tid er blevet samlet på, er der ingen grund til at tro, at personerne er bevidste om, at de indgår i datasættet. Og at de altså indtil nu har været genkendelige.»Our annotations confirm that faces are ubiquitous in ILSVRC and pose a privacy issue,« vurderer forskerholdet, der består af Kaiyu Yang, Jia Deng og Olga Russakovsky fra Princeton samt Jacqueline Yau og Li Fei-Fei ved Stanford.Wrongdoers og nøgne børnImageNet har været et signifikant datasæt for udvikling af deep learning-modeller til billedgenkendelse gennem ILSVRC, der i 2012 blev overbevisende vundet af AlexNet. Det var første gang, at et neuralt netværk vandt over de traditionelle metoder til objekt-genkendelse.Men over de seneste år er det også blevet åbenbart, at etik og privacy ikke stod ekstremt højt på dagsordenen, da datasættet blev skabt mellem 2007 og 2009.I 2019 illustrerede kunstprojektet ImageNet Roulette åbenlyse bias i billedernes tags, der leder til, at afroamerikanere kategoriseres som 'wrongdoer'og'offender'. Samme år kunne machine learning scientist Vinay Prabhu demonstrere, at datasættet rummede nøgne børn og materiale taget fra pornografiske hjemmesider.For nylig viste forskning, at systemer til billedegenerering - som iGPT og SimCLR - trænet på ImageNet var mere tilbøjelige til at generere mænd i jakkesæt og kvinder i bikinier.Ansigtsgenkendelse ikke nokAf samme grund af ImageNet været igennem en længere overhaling, der blandt andet skulle have sorteret ud i problematiske tags og billeder. Og nu er turen altså kommet til ansigter.Forskerholdet startede med at bruge Amazon Recognition til at detektere ansigter, hvilket viste sig at være et effektivt første skridt, selvom fejlraten i visse kategorier var for høj. For at komme af med falske positive (særligt i billeder af dyr) og falske negative (særligt i billeder af sport) blev resultatet sendt til menneskelig behandling gennem Amazon Mechanical Turk.
        </div>
        <footer>
          <em>Pro.ing.dk/datatech</em>
          &nbsp;·&nbsp; 2021-03-18
          &nbsp;·&nbsp; e82c40ea
          &nbsp;·&nbsp; #30
          <br>
          <br>
            <kbd data-tooltip="AI &amp; new algorithmic solutions">L02_AIALGO&nbsp;0.866</kbd>
            <kbd data-tooltip="AI, progress &amp; cybersecurity concerns">L05_AISECUR&nbsp;0.504</kbd>
            <kbd data-tooltip="hSBM 2_0">H_2_0&nbsp;0.571</kbd>
        </footer>
      </article>
      <article>
        <h4>Professionel kamerakvalitet i kompakt design</h4>
        <div>
          De nye kameraer i RX-serien byder på lynhurtig billedbehandling samt 40x Super Slow Motion, 4K-videooptagelser og meget mere.Sony lancerer nu de kompakte RX100 IV og RX10 II. De to nye modeller har begge verdens første &quot;1.0 type Stacked Exmor RS CMOS--sensor i med avanceret signalbehandling og indbygget DRAM-hukommelseschip.Den hurtige signalbehandling og DRAM-hukommelseschippen giver tilsammen fem gange hurtigere behandling af billeddata samt en række spændende funktioner, der tidligere kun fandtes i udvalgte, professionelle videokameraer.Du får for eksempel 40x super slow motion-optagelser i op til 1000 fps, en ultrahurtig &quot;Anti-Distortion Shutter- med en hastighed på op til 1/32000 af et sekund, 4K-videooptagelser og meget mere.Sony RX10 IIAlsidigt design og hurtig autofokusDet nye RX100 IV beholder det kompakte design, som kendetegner RX100-serien, så det passer stadig lige ned i lommen. RX100 IV er udstyret med et ZEISS Vario-Sonnar T 24-70mm F1.8-F2.8-objektiv svarende til 35mm. RX10 II beholder ligeledes samme design som det originale RX10, og det er udstyret med et ZEISS Vario-Sonnar T 24-200mm F2.8-objektiv svarende til 35 mm.PartnerAOD-logo_BOXstreg.pngDenne artikel er blevet til i samarbejde med Alt om DATA.Læs flere nyheder, artikler, guides og test påaltomdata.dkBegge kameraer har en ny indbygget XGA OLED Tru-Finder med høj kontrast. Du får her en opløsning på omkring 2,35 millioner punkter, hvilket sikrer visning og afspilning af indhold som det ser ud i virkeligheden. RX100 IV beholder den praktiske elektroniske søger med ZEISS T-coating fra RX100 III-modellen, som blev lanceret forrige år.Begge modeller byder desuden på en opgraderet &quot;Fast Intelligent--autofokus, der giver hurtig og præcis kontrast-detektions-autofokus af objekter i bevægelse på helt ned til 0,09 sekunder. Med den nye autofokus-algoritme udviklet af Sony kan kameraerne genkende og låse sig fast på objekter langt mere effektivt end tidligere modeller. Det eneste du skal gøre er at trykke udløserknappen halvt ned. De nye kameraer er Wi-Fi- og NFCT-kompatible, og de har begge adgang til det voksende udvalg af PlayMemories Camera-apps fra Sony.De to nye kameraer har begge en række brugerdefinerbare knapper og funktioner, og de kan justeres, så de passer til enhver fotografs ønsker.RX100 IV og RX10 II vil være tilgængelige fra midten af juli til priser på henholdsvis 9.200 og 12.500 kr.Sony RX100 IV. Foto: Sony.Sony RX100 IV. Foto: Sony.
        </div>
        <footer>
          <em>Jp.dk (Jyllands-Posten) (Abonnementsområde)</em>
          &nbsp;·&nbsp; 2015-06-27
          &nbsp;·&nbsp; e519baff
          &nbsp;·&nbsp; #31
          <br>
          <br>
            <kbd data-tooltip="AI &amp; new algorithmic solutions">L02_AIALGO&nbsp;0.783</kbd>
            <kbd data-tooltip="AI, progress &amp; cybersecurity concerns">L05_AISECUR&nbsp;0.719</kbd>
            <kbd data-tooltip="hSBM 2_0">H_2_0&nbsp;0.622</kbd>
        </footer>
      </article>
      <article>
        <h4>Sociale medier: AI-baseret ansigtsbytte på pornomodeller skal stoppes</h4>
        <div>
          Twitter, Pornhub og Gfycat accepterer ikke deepfakes, hvor kendte menneskers ansigter bliver klippet ind i pornografiske videoer ved hjælp af et nyt software-værktøj baseret på machine learningHarry Potter-skuespilleren Emma Watson og popstjernen Taylor Swift har i den seneste tid kunne se deres ansigter blive klippet ind i pornografiske videoer på internettet.Fænonometet er kendt som deepfake, og har for alvor taget fart siden december 2017, hvor en bruger på Reddit viste hvordan han bruger machine learning og neurale netværk til at bytte ansigter på kendte mennesker og pornoskuespillere.I januar publicerede en anden Reddit-bruger så værktøjet Fakeapp, som på en brugervenlig måde gør det muligt at bytte ansigter på mennesker i videoer og gifs.App'en er i skrivende stund blevet downloadet over 100.000 gange.Ansigtsbyttet bliver muligt fordi Reddit-brugerne har brugt en kombination af open source software der anvender machine learning, eksempelvis Google-s TensorFlowNu har Twitter, Pornhub, Gfycat og Discord alle meldt ud, at de fjerner alle former for deepfake-videoer.Det gør de med henvisning til, at de betragter det som hævnporno, når internetbrugere bruger værktøjet Fakeapp, til at klippe billeder af kendte menneskers ansigter ind i pornografiske videoer.» Vores brugerbetingelser tillader os at fjerne indhold som vi finder anstødeligt. Vi fjerner aktivt det type indhold,« skriver en talsperson fra Gfycat til The Verge.Også det store porno-site Pornhub er nu begyndt at fjerne de såkaldte deepfakes.»Vi accepterer ikke nogen former for indhold, hvor der ikke er givet samtykke på vores side, og vi fjerner denne type indhold så snart vi bliver opmærksomme på det,« skriver Pornhub tilMotherboard.På Reddit, som fænomenet er vokset ud af, og hvor FakeApp er publiceret, har man ikke forbudt deepfakes, med mindre der er tale om børnepornografi.Reddit-tråden med deepfakes har over 91.000 følgere og størstedelen af indlæggende handler om netop pornografi, hvor kendte menneskers ansigter er klippet ind.At bytte ansigter ud på billeder er ikke nyt. Det sociale medie Snapchat har haft funktionen i flere år, ligesom Faceapp gør dig i stand til at se en yngre eller ældre version af dig selv.Men med de nye machine learning-værktøjer er kvaliteten forbedret markant.
        </div>
        <footer>
          <em>Version2.dk</em>
          &nbsp;·&nbsp; 2018-02-07
          &nbsp;·&nbsp; e69bfa80
          &nbsp;·&nbsp; #32
          <br>
          <br>
            <kbd data-tooltip="Tech industry &amp; regulation">L02_BIGTEC&nbsp;0.523</kbd>
            <kbd data-tooltip="Facebook, fake news &amp; Trump">L05_FBTRUM&nbsp;0.517</kbd>
            <kbd data-tooltip="Social media &amp; well being">L10_SOMEWB&nbsp;0.558</kbd>
            <kbd data-tooltip="hSBM 2_0">H_2_0&nbsp;0.639</kbd>
        </footer>
      </article>
      <article>
        <h4>Verdensfirma på en mark i Stige</h4>
        <div>
          STIGE: STIGE: Ude bag et drivhus, ned ad en grusvej og på en mark med juletræer ligger et stort, hvidt hus.Her holder en masse biler og cykler, og hvis man ikke vidste det, var det ikke til at gætte, at her ligger en international virksomhed.For enden af grusvejen ligger nemlig det firma, som Shinta Darling Aarup og Esben Darling Meng etablerede tilbage i 2006: Colourbox.Colourbox er en kæmpe billeddatabase, hvor du kan finde billeder af stort set alt.Parret har en fortid i reklamebranchen, og deres erfaring var, at det var meget besværligt at købe billeder til netop dette.-Så lavede vi en streamingtjeneste.Her var der 65.000 billeder, forklarer Esben Darling Meng.I dag har Colourbox ti millioner billeder, og der kommer 10.000 nye til hver dag.-I begyndelsen lavede vi selv billederne og agerede selv modeller, så der findes mange billeder af os selv, hunden, katten, børnene og bedsteforældrene, siger Esben Darling Meng og smiler.Lokale ansigterHvis du kender Colourbox og bruger det, kan det være, du har set et ansigt derinde, som du også har set et sted i byen.Der findes nemlig en del folk fra Odense, der har ageret modeller til colourboxbilleder.-Vi tog på et tidspunkt nogle billeder i Billund Lufthavn, der skulle handle om flyforsinkelser.Der var jeg model, og der sad jeg på en bænk og kiggede på mit ur. Næste gang jeg så det billede, var det i en artikel om mænd og depression. Lidt efter var det i en artikel om, at kloge mænd har god sædkvalitet, siger Esben Darling Meng og griner.Når billeder fra Colourbox kan bruges frit til hvad som helst, så kan der ske mærkelige ting og sager.-For halvandet års tid siden blev vi ringet op af en kvinde.Hun havde læst Ugeavisen Odense, og deri var et billede af en mand og en kvinde i en intim situation. Kvinden spurgte, hvem den anden kvinde på billedet var, for manden var hendes kæreste, og han havde bestemt ikke fået lov til at lave den slags billeder. Det viste sig så, at manden sammen med sin ekskæreste havde lavet de billeder uden den nuværende kærestes viden, fortæller Esben Darling Meng.Fra Skibhus til Stige Da Shinta og Esben Darling etablerede firmaet, var det i en lejlighed på Døckerslundsvej i Skibhuskvarteret.Ret hurtigt fik de brug for mere plads, og så fandt de en gammel gård fra 1877 i Stige.-Vi skulle finde noget, der både kunne bruges til privatbolig og havde muligheder for ar udvide til erhverv. Det skulle ikke være noget, der havde nogle kedelige kontorer, og så skulle der være en god skole i nærheden, forklarer Esben Darling Meng.Sådan endte de i Stige.I dag er den gamle gård ikke at se. Bygningerne er topmoderne og vrimler med medarbejdere.Store tal på skærmen Hos Colourbox er der medarbejdere fra mange forskellige lande, blandt andet Pakistan og Rumænien, derfor er sproget engelsk, når alle taler sammen.På en skærm i mødelokalet popper der tal op hele tiden.Handlen af billeder på forskellige kontinenter overvåges tæt, og det er store tal -i euro vel at mærke.Alt i Colourbox måles hele tiden.-Det er algoritmer, der styrer, hvilket billede, der kommer frem, siger Esben Darling.Også faktorer som hvor brugeren kommer fra, hvilke højtider der er i landet og hvad der sidst er blevet søgt på, er med til at bestemme, hvilket foto, der kommer frem.Alt i alt går det meget godt hos Colourbox, og det er nok ogsågået meget hurtigere, end parret havde turde håbe på.-Vores succes er, at vi gode har layouts og gode priser. Både en erfaren IT-bruger og en uerfaren kan finde det, de søger på vores side. Og så var vi med fra start af, siger Esben Darling Meng.»Vi tog på et tidspunkt nogle billeder i Billund Lufthavn, der skulle handle om flyforsinkelser. Der var jeg model, og der sad jeg på en bænk og kiggede på mit ur. Næste gang jeg så det billede, var det i en artikel om mænd og depression. Lidt efter var det i en artikel om, at kloge mænd har god sædkvalitet.ESBEN DARLING MENG, MEDEJER OG -STIFTER AF COLOURBOX.Om ColourboxColourbox har billeder af stort set alt. Her kan reklamebureauer finde billeder, som de kan anvende i reklamer, og aviser kan finde et billede, der passer til en artikel, hvis der ikke findes et billede i forvejen. Det kunne foreksempel være, at der blev afholdt andespil i forsamlingshuset, og at der dertil skulle være et billede af en andesteg. Sådan et billede kan man finde på Colourbox.Colourbox har 50.000 fotografer tilknyttet. Nye fotografer skal sende 14 billeder ind, som bliver kigget igennem af en medarbejder på kontoret i Stige, og bliver de godkendt, kan fotografen levere så mange billeder, det skal være.For sit foto får fotografen 20 % af salgsprisen, dog minimum 20 cent.Alle billeder, der kommer ind fra fotografer, bliver set igennem. Colourbox sikrer sig, at der ikke er racistisk eller pornografisk materiale iblandt.En fotograf skal selv sørge for at sætte så mange søgeord på sine billeder som muligt. Dem, der får sat de bedst rammende søgeord på billedet, vil også få solgt flest billeder, da billederne så vil dukke op på flere søgninger.Når en kunde køber en billede på Colourbox, er billedet frit til brug af hvad som helst. Derfor kan det samme billede blive brugt til to meget forskellige ting.Colourbox har kunder i hele Verden. Ifølge Esben Darling Meng vil der på en måned kun være 25 lande i hele Verden, der ikke har købt et billede hos Colourbox.Colourbox har 200.000 aktive brugere, der køber billeder, og der er 75.000 besøgende hver dag.Colourbox har også en afdeling i Berlin, og alt i alt er der 30 ansatte.
        </div>
        <footer>
          <em>Erhvervsavisen Fyn</em>
          &nbsp;·&nbsp; 2014-11-18
          &nbsp;·&nbsp; e4aa7da5
          &nbsp;·&nbsp; #33
          <br>
          <br>
            <kbd data-tooltip="AI &amp; new algorithmic solutions">L02_AIALGO&nbsp;0.874</kbd>
            <kbd data-tooltip="AI, progress &amp; cybersecurity concerns">L05_AISECUR&nbsp;0.689</kbd>
            <kbd data-tooltip="hSBM 2_0">H_2_0&nbsp;0.69</kbd>
        </footer>
      </article>
      <article>
        <h4>Algoritme gør dig mere genkendelig på Facebook</h4>
        <div>
          Profilbilledet på de sociale medier eller på en datingprofil kan være afgørende for, om folk husker dig. Forskere fra MIT har skabt et stykke software, der klarer ærterne.Vi bliver bombarderet med ansigter på de sociale medier og i reklamer, men hvad er det egentlig, der afgør, om vi kan huske ansigtet bagefter?Det har en gruppe amerikanske forskere fra MIT luret på for at gøre det muligt selv at manipulere med sin genkendelighed, skriver gizmag.com .Forskerne er gået efter at kreere en algoritme, hvor man i et program kan uploade sit foto, hvorefter man får et nyt billede af sit ansigt, der står skarpere i folks hukommelse.Og der er tale om meget diskrete ændringer, så de færreste burde ane, at man er blevet 'photoshoppet' en smule.Det blev ifølge forskningsrapporten grebet an på den måde, at en række frivillige fra et crowdsourcing-netværk på Amazon til at begynde med fik lov at sidde og se på tusindvis af billeder af ansigter fra en database. Derefter blev 2.000 af dem udvalgt som værende 'genkendelige' ansigter - uden at de frivillige behøvede at sætte ord på hvorfor.Det foregik på den langstrakte måde, at der blev taget tusindvis af kopier af hvert foto, hvor der blev ændret en lille smule. Herefter blev de bedste udvalgt, hvorefter møllen kørte en gang til, indtil forskerne kunne deklarere, at softwaren nu var tilstrækkeligt mættet med oplysninger.Til sidst endte holdet op med 500 ansigter, som blev kørt igennem programmet, som nu var udstyret med lighedstrækkene for både de let genkendelige og de mindst genkendelige.Disse 500 ansigter blev sat over for de to forskellige former for lighed, så der i alt forelå tre fotos af hvert ansigt: Det oprindelige, det 'forbedrede' og det 'forværrede'.Nogle af ansigterne kan ses til højre, hvor det umiddelbart kan se ud som om, ansigterne er gjort lidt smallere, hvis de skal være mere genkendelige, og måske er kæben også lige blevet spidset lidt. Og er der ikke også kommet lidt mere 'trut' på læberne?Ifølge forskerne har det dog være vigtigt, at ændringerne skulle være meget diskrete og ikke måtte ændre grundlæggende ved ansigtet. Det måtte helst heller ikke gøre dem mindre attraktive.Ud over lige at gøre billedet på Facebook eller jobansøgningen lige dén snas lækrere, mener forskerne også, at softwaren omvendt kan bruges i f.eks. film-makeup, hvor man måske ønske, at statister skal træde lidt mere i baggrunden og ikke blive for genkendelige.Prøv selv forskernes lille hukommelsesspil her eller hent en del af forskernes kode, som kan forudsige genkendelighed her.Eksempler på fotos, der har været en tur igennem MIT-forskernes program. Billedet i midten er det oprindelige, mens varianten til højre er blevet gjort mere genkendelig og det til venstre mindre genkendelig. Foto: MIT.Eksempler på fotos, der har været en tur igennem MIT-forskernes program. Billedet i midten er det oprindelige, mens varianten til højre er blevet gjort mere genkendelig og det til venstre mindre genkendelig. Foto: MIT.
        </div>
        <footer>
          <em>Ing.dk (Ingeniøren)</em>
          &nbsp;·&nbsp; 2013-12-25
          &nbsp;·&nbsp; e42e47a7
          &nbsp;·&nbsp; #34
          <br>
          <br>
            <kbd data-tooltip="AI &amp; new algorithmic solutions">L02_AIALGO&nbsp;0.744</kbd>
            <kbd data-tooltip="AI, progress &amp; cybersecurity concerns">L05_AISECUR&nbsp;0.664</kbd>
            <kbd data-tooltip="hSBM 2_0">H_2_0&nbsp;0.706</kbd>
        </footer>
      </article>
      <article>
        <h4>Kun til arbejdsbrug: Microsoft lancerer nye Hololens-briller til 23.000 kr.</h4>
        <div>
          Synsvinklen var for lille. Hologrammerne var fine nok, men optrådte kun midt i synsfeltet - og forsvandt altså, når du drejede hovedet lidt.Sådan kan man kort opsummere modtagelsen af Microsofts første bud på briller med indbygget skærm, Hololens, der kom på gaden i foråret 2016. Men nu er Microsoft klar med en version 2 - og synsfeltet med hologrammer er blevet klart forbedret, lyder dommen hos tech-mediet The Verge.Hololens-brillerne er lavet til såkaldt augmented reality og mixed reality, altså hvor man både ser den virkelige verden og et digitalt lag. Et klassisk eksempel på, hvad man kan bruge teknologien til, er at stå i et tomt hus, men se det fyldt med møbler igennem brillerne.Med en startpris på 23.000 kr. for et par Hololens 2-briller har Microsoft dog besluttet ikke at prøve at sælge til almindelige forbrugere. Målgruppen er nu kun virksomheder og andre professionelle brugere, for eksempel fabriksarbejdere eller tekniske konsulenter, som rejser rundt og reparerer udstyr.For at få gjort synsfeltet større og Hololens-brillen nemmere at bruge, har Microsoft ændret markant på teknologien i brillerne. Kort fortalt opstår det gennemsigtige digitale billede ved at 'tegne' med laser i et særligt glaslag. Og de problemer, der normalt er med den type teknologi, er blevet fikset, bl.a. ved at have to kameraer, som holder øje med brugerens øjne.Dermed kan brillerne selv justere laserbilledet præcist efter, hvor øjenåbningerne er, i stedet for at man selv skal skrue og tilpasse, indtil billedet står klart. Og som ekstra bonus har Hololens 2-brillerne også styr på, hvor brugeren kigger hen. Det bliver for eksempel brugt til automatisk at scrolle en tekst opad, når brugeren har læst sig igennem et skærmbillede. Kameraerne bliver også brugt som ekstra sikkerhed, så man kan låse computeren i Hololens-brillen op med sine øjne, ligesom man bruger et fingeraftryk eller sit ansigt til at låse sin telefon op.En anden nyskabelse er, at brillerne som standard skal kommunikere med Microsofts cloud-tjeneste Azure og derfra få hjælp til at forstå, hvad den ser. Kameraer og sensorer registrerer hele tiden omgivelserne, så de digitale billeder for eksempel kan hvile på et bord eller hænge fra loftet. Men i version 1 af Hololens var det en simpel 3D-forståelse af omgivelserne. I den nye udgave skal brillerne kunne forstå, om den ser en sofa eller en stueplante ved hjælp af kunstig intelligens i skyen.Det gør det også muligt bedre at forstå brugerens håndbevægelser. I stedet for at skulle lære nogle få, særlige håndtegn, når man skal styre indholdet, er det i den nye version langt mere naturligt. Man kan for eksempel uden problemer trykke på knapper i den digitale verden med sin finger, rapporterer The Verges anmelder.Microsoft har siden lancering af Hololens i 2016 stået ret alene på markedet, men fik i efteråret 2018 en udfordrer, nemlig Magic Leap. Men de nye briller er foreløbigt kun blevet præsenteret. De kommer først til salg 'i løbet af 2019', fortæller Microsoft.
        </div>
        <footer>
          <em>Jyllands-posten.dk</em>
          &nbsp;·&nbsp; 2019-02-27
          &nbsp;·&nbsp; e718a9bd
          &nbsp;·&nbsp; #35
          <br>
          <br>
            <kbd data-tooltip="AI &amp; new algorithmic solutions">L02_AIALGO&nbsp;0.753</kbd>
            <kbd data-tooltip="AI, progress &amp; cybersecurity concerns">L05_AISECUR&nbsp;0.571</kbd>
            <kbd data-tooltip="hSBM 2_0">H_2_0&nbsp;0.685</kbd>
        </footer>
      </article>
      <article>
        <h4>VICTIM BLAMING FOR TV-LICENSEN</h4>
        <div>
          DR oplyser i et stort opsat journalistisk projekt om en ny form for digitale krænkelser. En række kvinder, der har delt billeder af sig selv på Instagram, er af ukendte gerningsmand blevet ' afklædt' på billederne med et særligt program, som via kunstig intelligens kan imitere, hvordan de ser ud uden tøj på. På nul komma fem genererer programmet en falsk krop, som passer til ansigtet på billedet.Og voila - fabrikeret hævnporno.Det er godt, grundigt og forstemmende journalistisk arbejde. Men man indigneres desværre yderligere af det her 100 procent victim blamende spørgsmål, som journalisterne stiller den 20-årige influencer Anna Briand, der har været udsat for forbrydelsen: »Du har over 200.000 følgere på Instagram og bruger dig selv meget via mange opslag, og du har selv optrådt i bikini og bh på din profil. Er det med til at lægge op til, at de falske billeder bliver lavet?« Buh, DR.
        </div>
        <footer>
          <em>Information</em>
          &nbsp;·&nbsp; 2021-04-16
          &nbsp;·&nbsp; e838b5a2
          &nbsp;·&nbsp; #36
          <br>
          <br>
            <kbd data-tooltip="Tech industry &amp; regulation">L02_BIGTEC&nbsp;0.51</kbd>
            <kbd data-tooltip="hSBM 2_0">H_2_0&nbsp;0.601</kbd>
        </footer>
      </article>
      <article>
        <h4>FBI køber ansigtsgenkendelse for 5,8 milliarder</h4>
        <div>
          Et nyt projekt skal gøre det nemmere for det amerikanske forbundspoliti FBI at lokalisere og identificere forbrydere.Det amerikanske forbundspoliti, FBI, er i fuld gang med at søsætte et nyt ambitiøst ansigtsgenkendelsessystem, som i 2014 skal finde forbrydere over hele USA. Projektet går under navnet Next Generation Idenfication (NGI) programme og koster ikke mindre end én milliard dollar eller 5,8 milliarder kroner.Med billeder af kriminelle i politiets database er det meningen, at det nye system skal kunne registrere en mistænkt, når han eller hun bevæger sig ind i synsfeltet af et sikkerhedskamera. Omvendt kan det også bruges til at finde en forbryder i databasen, hvis forbrydelsen er fanget på kamera.FBI har endnu ikke frigivet nogen detaljer om den algoritme, NGI bruger, men tidligere tests viser, at moderne algoritmer til ansigtsgenkendelse kan identificere en person med 92 procent sikkerhed i en database med 1,6 millioner billeder. Selv hvis personen er optaget på en skæv vinkel kan algoritmer generere en 3D-version af ansigtet og dreje det op til 70 grader, så det matcher billeder i databasen.Ud over ansigtsgenkendelse vil NGI også indeholde DNA-analyser, stemmegenkendelse og iris-scanner.Pilotprojektet kan kun søge efter tidligere kriminelle, men det er ukendt om FBI også vil tilføje billeder af andre, som ellers ikke har været i politiets søgelys.Moderne algoritmer til ansigtsgenkendelse kan identificere en person med 92 procent sikkerhed i en database med 1,6 millioner billeder..
        </div>
        <footer>
          <em>Ing.dk (Ingeniøren)</em>
          &nbsp;·&nbsp; 2012-09-14
          &nbsp;·&nbsp; e367038c
          &nbsp;·&nbsp; #37
          <br>
          <br>
            <kbd data-tooltip="AI &amp; new algorithmic solutions">L02_AIALGO&nbsp;0.614</kbd>
            <kbd data-tooltip="AI, progress &amp; cybersecurity concerns">L05_AISECUR&nbsp;0.647</kbd>
            <kbd data-tooltip="hSBM 2_0">H_2_0&nbsp;0.601</kbd>
        </footer>
      </article>
      <article>
        <h4>MIT fjerner datasæt, der har trænet AI-systemer til at være racistisk og sexistisk</h4>
        <div>
          Universitetet undskylder for datasæt, der er annoteret med nedsættende ord om kvinder og sorte. MIT, et af verdens ledende universiteter inden for AI-forskning, har permanent fjernet et billeddatasæt på grund af racisitisk og sexistiske labels. Det skriver The Register Datasættet blev skabt i 2006, og er siden blevet flittigt brugt til at udvikle modeller, der kan identificerer personer og objekter på billeder. Men da MIT sammensatte datasættet - 80 Million Tiny Images - blev det gjort uden nogen nærmere kuratering af de label, der er tilknyttet hvert billede, og som f.eks. skal lære et neuralt netværk, at billedet forestiller en cykel eller en bil. Tusindvis af billeder er nemlig annoteret med ord som 'luder', 'bitch' og 'pædofil'. Tilsvarende er mange billeder noteret med nedsættende ord rettet mod bestemte etniske grupper. Det viser en større kulegravning , som Vinay Prabhu, chief scientist hos UnifyID, og Abeba Birhane, ph.d.-studerende ved University College Dublin, har lavet. Forskere: Undskyld De problematiske labels er opstået fordi MIT i sin tid hentede navneord fra ord-databasen Wordnet, og brugte dem til automatisk at hente korresponderende billeder på søgemaskiner. I en meddelelse fra Antonio Torralba, Rob Fergus og Bill Freeman, der oprindeligt præsenterede datasættet , fortæller MIT-forskerne, at de ikke var klar over de problematisk labels. »Vi er meget berørte over det, og vi undskylder over for alle, der er blevet påvirket,«  skriver de. At manuelt rydde op i de 80 millioner billeder er ikke muligt, skriver forskerne. Og derfor fjerner de nu datasættet og opfordrer udviklere til at slette lokale kopier. Oprydning i ImageNet 80 Million Tiny Images er ligesom det ikoniske ImageNet-datasæt blevet brugt til at benchmarke AI-modeller. Men i modsætning til ImageNet har ingen altså tidligere lavet et grundig og kritisk gennemgang af indholdet. ImageNet - der er skabt af forskere ved Stanford og Princeton - har været signifikant for udvikling af deep learning-modeller til billedgenkendelse gennem den såkaldte ImageNet Challenge. Men heller ikke her stod etikken ekstremt højt på dagsordenen, da datasættet blev skabt mellem 2007 og 2009. Vinay Prabhu har tidligere demonstreret, at der er flere tvivlsomme billeder at finde i arkivet med over 14.000.000 billeder - foruden problematiske labels fandt Prabhu flere billeder af nøgne børn samt pornografisk materiale. Forskerne bag ImageNet satte i september sidste år gang i en større indsats, der blandt andet skal fjerne kønsmæssige og etniske bias i datasættet.
        </div>
        <footer>
          <em>Version2.dk</em>
          &nbsp;·&nbsp; 2020-07-03
          &nbsp;·&nbsp; e7c796cc
          &nbsp;·&nbsp; #38
          <br>
          <br>
            <kbd data-tooltip="AI &amp; new algorithmic solutions">L02_AIALGO&nbsp;0.99</kbd>
            <kbd data-tooltip="AI, progress &amp; cybersecurity concerns">L05_AISECUR&nbsp;0.719</kbd>
            <kbd data-tooltip="hSBM 2_0">H_2_0&nbsp;0.61</kbd>
        </footer>
      </article>
      <article>
        <h4>Machine learning sorterer agurker i Japan</h4>
        <div>
          Raspberry Pi, et kamera, et neuralt netværk og en lille kreds på et kort er de centrale dele i et selvlærende agurkesorteringsanlæg.Vi skriver på Version2 om mange mere eller mindre spektakulære løsninger inden for sundhed, kundeservice og politiarbejde der anvender machine learning.Systemdiagram over agurkesorteringsanlæggetMen teknologien kan såmænd også bruges til noget så banalt som at sortere agurker - agurketid eller ej.Japaneren Makoto Koike, der er tidligere udvikler af indlejrede systemer, har bygget et agurke-sorteringsanlæg, der gør brug af Googles TensorFlow machine learning teknologi for at spare hans forældre, der ejer grøntsagsproduktionen, for en masse arbejde. Det fortæller Engadget.Systemet bruger Raspberry Pi 3 med et kamera til at tage billeder af grøntsagerne og sende optagelserne til et lille TensorFlow neuralt netværk, hvori identificeres som agurker.Herefter sendes billeder til en Linux-server, der klassificerer agurkerne efter farve, form og størrelse. Et kort med en lille kreds, Arduino Micro, styrer herefter selve sorteringen, mens en Windows-pc sikrer at det neurale netværk løbende optrænes ved hjælp af billeder.Det er ikke et perfekt system, i hvert fald lige nu. Til trods for at 7000 billeder er høvlet igennem systemet.Makoto Koike anslår, at det tager omkring 2-3 dage at træne den intelligente software op i sortering, hvilket dog sker billeder meget meget lav opløsning (80 x 80 pixels).Uanset at resultatet ikke er perfekt, antyder eksemplet en fremtid, hvor robotbaseret landbrugsudstyr kan håndterer mange opgaver, der tidligere krævede menneskehånd og -øjne.
        </div>
        <footer>
          <em>Version2.dk</em>
          &nbsp;·&nbsp; 2016-09-05
          &nbsp;·&nbsp; e5de815a
          &nbsp;·&nbsp; #39
          <br>
          <br>
            <kbd data-tooltip="AI &amp; new algorithmic solutions">L02_AIALGO&nbsp;0.988</kbd>
            <kbd data-tooltip="AI, progress &amp; cybersecurity concerns">L05_AISECUR&nbsp;0.942</kbd>
            <kbd data-tooltip="hSBM 2_0">H_2_0&nbsp;0.642</kbd>
        </footer>
      </article>
      <article>
        <h4>Svenske elge bliver nye linselus</h4>
        <div>
          GÄVLEBORG: Når elgene bevæger sig rundt i de svenske skove, skal de fremover vænne sig til, at mennesker kigger med.Der er nemlig ved at blive sat kameraer op i skovene i Gävleborg i Sverige som en del af et EU-projekt.De skal samle billeder ind af elgene, så dyrene på den måde fremover kan tælles uden hjælp fra jægere.Kameraerne bliver aktiveret af varme og bevægelse. Ved hjælp af kunstig intelligens skal det så være muligt at genkende elgene og på den måde få oplysninger om elgbestanden i skovene. Elgen skal også kunne genkendes på andre biometriske data som størrelse eller afstand mellem krop og hoved. I fremtiden skal teknologien også bruges til at kortlægge andre typer dyr. / ritzau/TT.
        </div>
        <footer>
          <em>Dagbladet Køge</em>
          &nbsp;·&nbsp; 2021-01-04
          &nbsp;·&nbsp; e80ef119
          &nbsp;·&nbsp; #40
          <br>
          <br>
            <kbd data-tooltip="AI &amp; new algorithmic solutions">L02_AIALGO&nbsp;0.82</kbd>
            <kbd data-tooltip="AI, progress &amp; cybersecurity concerns">L05_AISECUR&nbsp;0.746</kbd>
            <kbd data-tooltip="New technologies">L80_NEWTEC&nbsp;0.523</kbd>
            <kbd data-tooltip="hSBM 2_0">H_2_0&nbsp;0.664</kbd>
        </footer>
      </article>
      <article>
        <h4>'Drømmende'computer rekonstruerer George Clooney fra ny vinkel</h4>
        <div>
          Computere efterligner menneskets forestillingsevne i ny metode udviklet i samarbejde mellem danske og amerikanske forskere.Målet med forskningen er, at billedgenkendelse bliver stærkere, når kun få billeder er til rådighedMennesket overgår den dyreste og mest komplicerede computer, når det kommer til at improvisere læring og finde kreative løsninger. Derfor forsøger forskningen med den såkaldte Deep Learning at efterligne funktioner i vores hjerner.Deep Learning vinder frem i arbejdet med kunstig intelligens. Forskere ved Danmarks Tekniske Universitet samarbejder med det amerikanske MIT (Massachusetts Institute of Technology) med at forbedre kunstig intelligens ved at få computere til at forestille sig nye billeder på baggrund af oplysninger fra billeder, den er blevet fodret med.»Vi kalder det, at computeren drømmer, fordi billeddannelsen i computeren minder om, når mennesker i drømme er i stand til at forestille sig noget, vi ikke har set i virkeligheden«, siger Lars Kai Hansen, der er professor ved Danmarks Tekniske Universitet, og som sammen med postdoc Søren Hauberg står bag den danske del af det nye resultat, der netop er blevet præsenteret på konferencen -Artificial Intelligence and Statistics- i Cadiz i Spanien.Drømmer om George ClooneyForskerne har eksempelvis fodret computere med billeder af George Clooneys ansigt set forfra. De har udviklet algoritmer og statistiske modeller, der resulterer i, at computere kunne konstruere et billede af skuespillerens ansigt fra en lidt anden synsvinkel, end synsvinklen på de billeder, den blev fodret med.Selvom det kan lyde kuriøst, sigter projektet på konkrete anvendelsesmuligheder.»Målet med forskningen er, at billedgenkendelse bliver stærkere, når kun få billeder er til rådighed. Dette vil specielt få stor indflydelse på analyse af medicinske scanningsbilleder, som vi oftest kun har i meget begrænset antal«, siger Lars Kai Hansen.Et velkendte eksempel på Deep Learning var, da Google Deep Mind for nylig vandt over den erfarne topspiller Lee Sedol i computerspillet GO. Her var resultaterne, som ofte med Deep Learning, opnået på baggrund af, at computeren var blevet fodret med meget store mængder af data. I resultaterne fra det dansk-amerikanske samarbejde, som netop er blevet præsenteret, kan computerne arbejde ud fra færre billeder.Forskernes efterligninger af menneskets forestillingsevne er i sin spæde begyndelse. For eksempel kan computeren endnu kun dreje George Clooneys ansigt en lille smule   og der er endnu lang vej, før computeren eksempelvis vil kunne vise skuespilleren i profil på baggrund af det billedmateriale, forskerne her har brugt, vurderer Lars Kai Hansen.forsøgsperson. Fotomontage af George Clooney, som er et af flere ansigter i billeddatasættet -Labelled Faces in The Wild-, der bliver brugt til forskning i ansigtsgenkendelse af danske og amerikanske forskere i ny metode til at forbedre kunstig intelligens. Fotomontage. Foto: AP.forsøgsperson. Fotomontage af George Clooney, som er et af flere ansigter i billeddatasættet -Labelled Faces in The Wild-, der bliver brugt til forskning i ansigtsgenkendelse af danske og amerikanske forskere i ny metode til at forbedre kunstig intelligens. Fotomontage. Foto: AP.
        </div>
        <footer>
          <em>Politiken.dk</em>
          &nbsp;·&nbsp; 2016-05-10
          &nbsp;·&nbsp; e5b3ea03
          &nbsp;·&nbsp; #41
          <br>
          <br>
            <kbd data-tooltip="AI &amp; new algorithmic solutions">L02_AIALGO&nbsp;0.98</kbd>
            <kbd data-tooltip="AI, progress &amp; cybersecurity concerns">L05_AISECUR&nbsp;0.668</kbd>
            <kbd data-tooltip="hSBM 2_0">H_2_0&nbsp;0.596</kbd>
        </footer>
      </article>
      <article>
        <h4>Ét kamera skyld i bøder på 29 mio kr</h4>
        <div>
            I Australien har politiet intelligente trafik-kameraer, der kan fange ulovlig mobiltelefon-brug. Rigspolitiet i Danmark synes det lyder spændende. - Storartet. - Vi har brug for 10 kameraer yderligere rundt omkring i Sydney. - Så kan politiet bruge tiden på noget andet - som f.ex. kan de fange forbrydere. Ingen håndholdt, intet problem Sådan skriver Leigh G i en af de 51 kommentarer, der er skrevet på Sydney Morning Heralds Facebook-profil om det intelligente trafik-kamera på byens M4-ringvej har fanget mere end 20.000 bilister, der har brugt mobilen. Det er nemlig sket på bare fire måneder ved hjælp af særlige algoritmer og Leigh er ikke den eneste, der er positiv: - Ingen håndholdt mobil i bilen = intet problem, skriver Triskele R og Shrileen B synes også, der er en let måde at indgå bøder på. - Gør nu bare det rigtige, og lad vær med at udsæt mit og dit eget liv for fare, skriver Shrileen. Bøder for 64 mio I Sydney begyndte man at sætte kameraer op i trafikken i slutningen af 2019, og i alt har de seks kameraer indbragt cirka 64 millioner kr i bøder. Mobil-synder kameraet står for næsten halvdelen af det beløb og i Danmark følger Rigspolitiet spændt med: Vi følger det med interesse - Med hensyn til de intelligente kameraer, så følger vi udviklingen på området med interesse, da det er et spændende område. .- Uopmærksomhed i trafikken er et af tre store fokus områder for færdselspolitiet, da uheldsraten umiddelbart er meget høj på det område, skriver Christian Berthelsen, politiassistent i Rigspolitiet til nationen! Sidste år blev der udstedt 24.000 bøder for brug af håndholdt mobiltelefon og andre former for telekommunikationsudstyr. I 2013 var det tal oppe på 48.000.
        </div>
        <footer>
          <em>EkstraBladet.dk/Plus</em>
          &nbsp;·&nbsp; 2020-08-19
          &nbsp;·&nbsp; e7d80b46
          &nbsp;·&nbsp; #42
          <br>
          <br>
            <kbd data-tooltip="AI &amp; new algorithmic solutions">L02_AIALGO&nbsp;0.627</kbd>
            <kbd data-tooltip="AI, progress &amp; cybersecurity concerns">L05_AISECUR&nbsp;0.509</kbd>
            <kbd data-tooltip="hSBM 2_0">H_2_0&nbsp;0.611</kbd>
        </footer>
      </article>
      <article>
        <h4>Googles Captcha-afløser hjælper computeren med at lære at fortolke billeder</h4>
        <div>
          Mennesker er stadig bedre end computere til at genkende billeder. Men flere forskerhold arbejder på at gøre kål på menneskets forspring.Af Jesper Stein SandalGoogle har sagt farvel til den klassiske CAPTCHA, som skulle skille mennesker fra automatiske scripts ved at bede dem tyde en sløret tekststreng. Afløseren bruger billeder, og det skal hjælpe Googles billedgenkendelsessoftware, skriver New Scientist .I den nye udgave af Googles CAPTCHA-udfordring vil et menneske som regel kunne genkendes alene ud fra visse kendetegn som eksempelvis musens bevægelser, som kan aflæses af hjemmesiden. Men hvis Googles algoritme kommer i tvivl, kan brugeren blive bedt om eksempelvis at identificere et billede, som skiller sig ud i en gruppe af billeder.CAPTCHA-udfordringer bruges blandt andet til at forhindre automatiske scripts, som kan registrere sig på hjemmesider og spamme debattråde med reklamer.De gamle udgaver er ikke længere så pålidelige, fordi computerne er blevet bedre til at afkode de slørede tekster. Men computere er stadig meget mere upålidelige til eksempelvis at se, om et lille pelset dyr er en kat eller en kanin. Det er mennesker til gengæld ekstremt gode til.Det er det, Google nu udnytter, men ud over at beskytte hjemmesider mod spam, så giver teknologien også Google værdifulde datasæt. Hver gang et menneske har hjulpet med at identificere et billede ud af en mængde, så kan Google bruge det samme datasæt til oplæring af billedanalysealgoritmer.Det vil blandt andet kunne hjælpe Google med at forbedre billedsøgning, hvor Google i dag er afhængig af enten tekst i sammenhæng med et billede eller mønstergenkendelse, som har en vis fejlrate.Google arbejder også ifølge MIT Technology Review på et billedanalyseværktøj, som bruger vektormatematik til at sætte ord på billeder. Det er en videreudvikling af en teknik, som Google i forvejen bruger til tekstoversættelse mellem sprog, men den kan også bruges til billeder.Et forskerhold ved Stanford University arbejder ligeledes på et sæt af maskinlæringsalgoritmer, der kan sættes ord på billeder . Ved hjælp af en database bestående af 14 millioner objekter udtrykt ved hjælp af vektorer, kan algoritmerne genkende objekterne i billeder og sammensætte en beskrivelse. Det gør det eksempelvis muligt at beskrive indholdet af et billede som eksempelvis 'en kat sidder på et tastatur'.Denne type avanceret analyse af indholdet i billeder er især interessant i forhold til at kunne søge i og kategorisere de millioner af billeder og videoer, som distribueres på internettet, som ofte ikke er forsynet med mange oplysninger om, hvad de forestiller.
        </div>
        <footer>
          <em>Version2.dk</em>
          &nbsp;·&nbsp; 2014-12-11
          &nbsp;·&nbsp; e4b7abfc
          &nbsp;·&nbsp; #43
          <br>
          <br>
            <kbd data-tooltip="AI &amp; new algorithmic solutions">L02_AIALGO&nbsp;0.707</kbd>
            <kbd data-tooltip="AI, progress &amp; cybersecurity concerns">L05_AISECUR&nbsp;0.654</kbd>
            <kbd data-tooltip="hSBM 2_0">H_2_0&nbsp;0.688</kbd>
        </footer>
      </article>
      <article>
        <h4>Amazon planlægger intens kameraovervågning af medarbejdere</h4>
        <div>
          Amazon vil overvåge medarbejdere med kameraer i virksomhedens leveringsbiler. Ifølge tech-giganten skal data skal bruges til evaluere førernes arbejde og forbedre deres sikkerhed.Amazon planlægger at installere kameraer i sine leveringsbiler, så man bedre kan holde øje med medarbejderne, når de leverer pakker.Det skriver The VergeSystemet leveres af virksomheden Netradyne, som står bag Driveri - en platform, der bruger kameraer og kunstig intelligens til at analysere førerne. Systemet indsamler data, der bruges til at evaluere de enkelte førere, samtidig med, det under kørslen giver føreren beskeder som »distraheret kørsel«  og »sæt venligst farten ned« .»Vi leder altid efter innovative måder at styrke vores føreres sikkerhed på. Det er derfor, vi har indgået et samarbejde med Netradyne for at forbedre førerens oplevelse,«  siger Karolina Haraldsdottir, Chef for Amazons 'last-mile safety', i en marketingsvideo.Kameraerne optager hele tiden, og data sendes direkte til Amazon. Dog kan føreren deaktivere kameraet - men kun når bilen er slukket.
        </div>
        <footer>
          <em>Version2.dk</em>
          &nbsp;·&nbsp; 2021-02-04
          &nbsp;·&nbsp; e81c9937
          &nbsp;·&nbsp; #44
          <br>
          <br>
            <kbd data-tooltip="AI &amp; new algorithmic solutions">L02_AIALGO&nbsp;0.53</kbd>
            <kbd data-tooltip="AI, progress &amp; cybersecurity concerns">L05_AISECUR&nbsp;0.585</kbd>
            <kbd data-tooltip="hSBM 2_0">H_2_0&nbsp;0.584</kbd>
        </footer>
      </article>
      <article>
        <h4>Twitter indrømmer: Billedbeskæring med AI var diskriminerende</h4>
        <div>
          Det var korrekt, da en Twitter-bruger påpegede, at tjenestens algoritme til billedbeskæring foretrak hvide ansigter frem for sorte.Twitter fik sidste år kritik, da en algoritme til at bestemme, hvordan billeder skal beskæres, viste sig at være mere tilbøjelig til at skære sorte personer fra, og lade hvide personer blive i billedet.Twitter-brugeren @bascule testede den gang beskæring med et billede, der øverst har et portræt af den amerikanske senator Mitch McConnell, som er hvid, og nederst tidligere præsident Barack Obama.Her valgte Twitters beskæring fotoet af den hvide senator. I et senere tweet skrev @bascule:»Flere, der svarede på min tweet, har prøvet alle mulige grundige slags 'tweaks' for at få andre resultater. Det forbigår min originale pointe med mit eksperiment: Jeg prøvede bare at finde to nogenlunde ens udseende billeder og (hånden på hjertet) ville se hvad resultatet var.« Et bestemt 'tweak' - nemlig at øge kontrasten på det ene billede - afslører problemet, mente Twitter-brugeren, som testede med to billeder af Barack Obama, hvor det ene har højere kontrast, og er mere sort:»Algoritmen synes at foretrække en lysere Obama frem for en mørkere.« Nu har selskabets egne interne undersøgelser sat tal på problemet, skriver vores søstermedie DataTech , der citerer Zdnet.En rapport fra Twitter viser en forskel på 4 procent mellem hvide mennesker og sorte mennesker. Værre ser det ud mellem køn, hvor kvinder foretrækkes over mænd med en forskel på 7 procent. Til gengæld fandt undersøgelsen ikke tegn på, at modellen foretrækker bestemte kropsdele, når billeder skal beskæres.Det sociale medie valgte allerede sidste at justere den automatiske beskæring af billeder og i højere grad give brugere mulighed for at vælge.
        </div>
        <footer>
          <em>Version2.dk</em>
          &nbsp;·&nbsp; 2021-05-25
          &nbsp;·&nbsp; e847da81
          &nbsp;·&nbsp; #45
          <br>
          <br>
            <kbd data-tooltip="Tech industry &amp; regulation">L02_BIGTEC&nbsp;0.529</kbd>
            <kbd data-tooltip="Facebook, fake news &amp; Trump">L05_FBTRUM&nbsp;0.527</kbd>
            <kbd data-tooltip="hSBM 2_0">H_2_0&nbsp;0.605</kbd>
        </footer>
      </article>
      <article>
        <h4>Lyde genskabes ud fra vibrationer i chipspose</h4>
        <div>
          Amerikanske forskere har fundet ud af, at man kan rekonstruere lyd ud fra vibrationer fra objekter. Den nye opfindelse skal blandt andet bruges til at opklare af forbrydelserForskere fra Massachusetts Institute of Technology, MIT, har i samarbejde med Microsoft Research og Adobe udviklet en algoritme, de kalder 'Den visuelle mikrofon'. Den gør det muligt at lytte til andres samtaler, udelukkende ved at analysere videomontager af vibrerende objekter, skriver Videnskab.dk.Projektet, som forskerne kalder 'The Visual Microphone: Passive Recovery of Sound from Video' er blevet publiceret i tidsskriftet ACM Transactions on Graphics.I forsøget benyttede forskerne sig blandt andet af vibrationerne fra en chipspose, planteblade og høretelefoner, der lå på et bord, til at afkode lyden fra en person, som sang sangen 'Mary had a little lamb'.Se video af det spektakulære forsøg på Videnskab.dkForskerne optog blandt andet chipsposens vibrationer i en afstand på 15 meter bag et lydisoleret glas, og det med stor succes.Det kan måske være svært at forstå, hvordan man kan rekonstruere lyd ud fra videooptagelser. Forskningsleder og kandidatstuderende Abe Davis fra MIT forklarer:- Når en lyd rammer et objekt, begynder objektet at vibrere. Denne vibrerende bevægelse skaber et lille subtilt visuelt signal, som normalt er usynligt for det blotte øje. Disse informationer har vi ikke før vidst eksisterede, udtaler Abe Davis i en pressemeddelelse fra MIT.Læs også på Videnskab.dk: Forskere: Fingre afslører længden på penisForskerne benyttede sig i de fleste tilfælde af et højshastighedskameraer til at optage vibrationerne fra objekterne. I sådanne kameraer bør antallet af billeder pr. sekund, Frames Per Second (FPS), være højere end frekvensen på lydsignalet.Kameraene, som forskerne anvendte i forsøget, kunne optage 2.000-6.000 FPS. Til sammenligning optager de fleste smartphones omkring 60 FPS.Den nye teknologi skal blandt andet bruges inden for efterforskningsverdenen, hvor optagelsesudstyr og aflytninger allerede benyttes.Andre artikler fra Videnskab.dk:Forskere aflurer Bruce Lees berømte trick: Sådan imponerer du tøserneSe en Coca Cola-dåse blive fuldstændig opløstFascinerende billeder: Jorden set fra rummetFoto: Linda Johansen.Foto: Linda Johansen.
        </div>
        <footer>
          <em>EkstraBladet.dk</em>
          &nbsp;·&nbsp; 2014-08-08
          &nbsp;·&nbsp; e485b9c3
          &nbsp;·&nbsp; #46
          <br>
          <br>
            <kbd data-tooltip="AI &amp; new algorithmic solutions">L02_AIALGO&nbsp;0.983</kbd>
            <kbd data-tooltip="AI, progress &amp; cybersecurity concerns">L05_AISECUR&nbsp;0.706</kbd>
            <kbd data-tooltip="hSBM 2_0">H_2_0&nbsp;0.557</kbd>
        </footer>
      </article>
      <article>
        <h4>EUC Syd jager meteorer fra hustag i Sønderborg</h4>
        <div>
          DEL Af Bent Christensen, express5@jfmedier.dk 16. oktober 2019 kl. 08:39Sønderborg: Som en del af et stort verdensomspændende netværk har EUC Syd dette efterår opsat et webcam på et af tagene i Sønderborg. Der foregår nu en uafbrudt monitorering af himlen for at spore meteoritters oprindelse og fald, og det foregår i et samarbejdet mellem EUC Syd, Den Tyske Skole i Sønderborg og organisationen fripon.org.EUC Syd er som de første fra Skandinavien med i projekt Vigie Ciel (Sky Watch) og er indtil videre det nordligste observationspunkt.Meningen er, at alle data og billeder fra samtlige deltageres digitale kameraer løbende bliver analyseret via nogle algoritmer. Når der optræder asymmetri i meteoritters bevægelse hen over himlen, kan man meget præcist dække nedfaldene, da man har observationer og data fra adskillige kameraer med optagelser fra mange forskellige vinkler. Således forventes det, at man kan spore og finde meteoritter efter nedfald.Både Den Tyske Skole og EUC Syd har planer om at kunne bruge de forskellige observationer, data og tekniske udregninger i undervisningen.
        </div>
        <footer>
          <em>Ugeavisen-soenderborg.dk</em>
          &nbsp;·&nbsp; 2019-10-16
          &nbsp;·&nbsp; e76654ee
          &nbsp;·&nbsp; #47
          <br>
          <br>
            <kbd data-tooltip="AI &amp; new algorithmic solutions">L02_AIALGO&nbsp;0.918</kbd>
            <kbd data-tooltip="AI, progress &amp; cybersecurity concerns">L05_AISECUR&nbsp;0.643</kbd>
            <kbd data-tooltip="hSBM 2_0">H_2_0&nbsp;0.651</kbd>
        </footer>
      </article>
      <article>
        <h4>Her er resultatet, når neurale netværk får frit spil på fotos</h4>
        <div>
          Et billede af en ridder, udsat for 'dyregenkendelse' i et neuralt netværk. Netværket er blevet bedt om at finde dyr i billedet, hvorefter de elementer, netværket har fundet, er blevet forstærket. Derefter er processen kørt igen og igen. (Foto: Google)Neurale netværk, der er trænet til at se på billeder, kan faktisk også skabe billeder. Det viser et interessant   nogle vil måske sige foruroligende   eksperiment fra Googles forskningsafdeling.Af 11. jul 2015 kl. 14:00Internetgiganten Google bruger neurale netværk til billedgenkendelse, bl.a. i sin nye fototjeneste Google Photos. Men hvor imponerende det end kan være, at disse netværk kan 'se' på en mængde billeder og finde dem med en 'hund' eller en 'banan', så står det faktisk ikke helt klart for forskerne, hvordan det foregår.Derfor har softwareingeniører hos Google prøvet at vende processen på hovedet: I stedet for f.eks. at vise netværket millioner af billeder af bananer, indtil det er i stand til at genkende én, så har de taget et gråmeleret billede bestående af tilfældig støj og bedt netværket om at se efter bananer.Når netværket mente at have set noget, der lignede en banan, forstærkede forskerne det fundne en lille smule og sendte så det nye billede ind i netværket igen   og så videre, og så videre, indtil der opstod billeder, der begyndte at ligne noget:Eksempler på, hvad Googles neurale netværk kan få ud af et billede med grå støj, hvis det bliver bedt om at se efter forskellige genstande som f.eks. søstjerner, skruer eller koantiloper (hartebeest). (Foto: Google)Som forskerne skriver i deres blogindlæg om forsøget , er det lærerigt at vende processen på hovedet, fordi det kan afsløre, om netværket nu også fokuserer på de rigtige karakteristika, når det kigger efter en banan, en søstjerne eller en skrue.Frie fortolkningerNeurale netværk består af flere 'lag', der i forbindelse med billedgenkendelse ser efter stadigt mere komplicerede elementer. Hvis man beder et af de lavere lag om at 'gå i selvsving' på billeder, der rent faktisk forestiller noget, vil de producere fortolkninger, der typisk indeholder forstærkede linjer eller ornamenteringer af forlægget.Til venstre et originalfoto af Zachi Evenor, til højre behandlet i Googles neurale netværk af softwareingeniør Günther Noack. (Foto: Google)Er det i stedet et af de højere lag i netværket, der bearbejder et billede, kan der opstå mere komplekse elementer eller objekter. Som Google-ingeniørerne forklarer i blogindlægget beder de igen bare netværket om 'mere af det samme': 'Hvis en sky har en smule lighed med en fugl, vil netværket få den til at ligne en fugl endnu mere. Det vil få netværket til at genkende fuglen endnu nemmere næste gang, indtil et meget detaljeret billede af en fugl kommer frem   tilsyneladende ud af ingenting.' Som i eksemplet herunder:Fugle opstået ud af 'ingenting' i et foto af en blå himmel med skyer. (Foto: Google)Forskerne mener, at teknikken kan give en kvalitativ forståelse af det abstraktionsniveau, et givet lag i et neuralt netværk har udviklet i sin forståelse af billeder. De kalder teknikken 'inceptionism' og har samlet et helt galleri over eksempler på, hvilke billeder, der kan hentes ud af et neuralt netværk. Som f.eks. denne gruppe af billeder, der alle er skabt ved at lade et neuralt netværk oplært af MIT Computer Science and AI Laboratory til at genkende steder tygge på grå billeder af tilfældig støj, rigtig mange gange:Fantasier over bygninger   skabt ud fra grå støj af et neuralt netværk. (Foto: Google)Efter det første blogindlæg om deres eksperimenter har forskerne nu valgt at lægge deres kode ud open source, så andre kan forsøge at eftergøre dem kunsten   og de opfordrer folk, der gør det, til at tagge deres billeder #deepdream.
        </div>
        <footer>
          <em>Ing.dk (Ingeniøren)</em>
          &nbsp;·&nbsp; 2015-07-11
          &nbsp;·&nbsp; e51ee60c
          &nbsp;·&nbsp; #48
          <br>
          <br>
            <kbd data-tooltip="AI &amp; new algorithmic solutions">L02_AIALGO&nbsp;0.907</kbd>
            <kbd data-tooltip="AI, progress &amp; cybersecurity concerns">L05_AISECUR&nbsp;0.727</kbd>
            <kbd data-tooltip="hSBM 2_0">H_2_0&nbsp;0.648</kbd>
        </footer>
      </article>
      <article>
        <h4>Her er Huaweis første mobil i Danmark i år</h4>
        <div>
          Men Huawei spytter nye modeller i stor hast. Derfor er det heller ikke overraskende, at årets første mobil allerede er blevet lanceret.Solidt batteriNyheden hedder og er bygget til de yngre mobilbrugere. Telefonen har et 6,21 tommers display i størrelsesforholdet 19.5:9, 64 gigabyte lagerplads og et pænt batteri på 3400 mAh.Displayet er på 415ppi og FHD+ med en opløsning på 2340 x 1080 pixels. Og selvom modellen koster 1899 kroner i vejledende pris, er der blevet plads til ganske habile kameraer.Sådan er kameraerneDer er to kameraer på bagsiden på henholdsvis 13 og 2 megapixels. Det sidstnævnte bruges ifølge Huawei til at 'øge detaljegraden, så du kan skyde de helt store vidder og landskaber og mere detaljerede portrætbilleder.'Selfiekameraet er på 8 megapixels. Og der er som i P20- og Mate-serien indbygget kunstige intelligens, som kan gøre det nemmere at skyde bedre billeder.Du kan dog alene optage video i op til 1080p og ikke 4K.Ekstra hukommelseModsat flere dyrere udgaver af Huaweis telefoner kan du bruge et ekstern hukommelseskort på op til 512 gigabyte i P Smart 2019.Opladningen sker via et microUSB-kabel, der er normalt hovedtelefonstik i modellen, og der følger en hovedtelefon med i salgskassen.Modellen er allerede til salg hos Elgiganten og Power i farverne Midnight Black og Aurora Blue. Den kommer i næste kvartal til salg hos andre forhandlere.
        </div>
        <footer>
          <em>Ekstrabladet.dk</em>
          &nbsp;·&nbsp; 2019-01-12
          &nbsp;·&nbsp; e70a5e50
          &nbsp;·&nbsp; #49
          <br>
          <br>
            <kbd data-tooltip="AI &amp; new algorithmic solutions">L02_AIALGO&nbsp;0.644</kbd>
            <kbd data-tooltip="AI, progress &amp; cybersecurity concerns">L05_AISECUR&nbsp;0.567</kbd>
            <kbd data-tooltip="hSBM 2_0">H_2_0&nbsp;0.602</kbd>
        </footer>
      </article>
    </main>
  </body>
</html>